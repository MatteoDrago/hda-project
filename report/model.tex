% !TEX root = template.tex

\section{Processing Pipeline}
\label{sec:processing_architecture}

We start off our analysis by preprocessing the collected signals within the MATLAB framework: we chose that because it makes it simple to deal with matrices. What we do in this first step is then to import the data collected by sensors, which are given as .dat files, select the signals from on-body sensors and discard the others, replacing the missing values by means of interpolation and, at last, store them as .mat files.
What we do next is to import the stored data, this time using python, and prepare the matrices for the classification task: this consists of concatenating the data, segmenting it into windows, scaling the signals and other common steps.
Once the data is ready to be classified, a model is defined and trained on the available data. This is done for both the locomotion activity and gestures recognition, i.e. with two different sets of labels. This system, which is forced to learn also the null class together with the actual movements, is then compared to a different system where two models are deployed: the first one has the only purpose of detecting activity, while the second classifies the activity, if present.

\section{Signals and Features}
\label{sec:model}

The signals that we use to perform HAR are the ones collected in the OPPORTUNITY activity recognition dataset. The measurement setup is then the one presented in \cite{Roggen2010} and \cite{Chavarriaga2015}. Our analysis though is based only on on-body sensor signals, which means that we kept the signals of only a subset of the available sensors: discarding the other signals then, we ended up with 113 signals. During the preprocessing, we discarded also 3 of them, belonging to the same physical device, because there weren't any measurements in most of the cases. This led us to work on 110 signals.
Since we noticed that the almost all the sensors, at the beginning and at the end of the measurement sessions, have sequences where there isn't any sample recorded, we decided to discard the head and the tail of each session, in such a way that we start and stop with all the measurements being registered.
This choice was made also to facilitate interpolation. In MATLAB we perform a splines interpolation, which uses a cubic polynomial. The decision of cutting head and tail prevented our code from interpolating a piece of signal which has only one "edge".
Then, to perform classification on the data of one subject, we stacked sessions ADL 1 to 3 and Drill to create our training set, and then ADL4 and ADL5 as test set. In some cases, interpolation leaves entire columns to NaN because it isn't provided any data to interpolate those values. We solved the problem by setting to 0 those entire columns.
Subsequently we scaled the signals by subtracting their means and dividing by their variance (or sigma?). After this, data is shaped into windows of 15 samples (500 ms) with a stride of 5 samples. The approach that we used to segment the data was then the sliding window introduced above. To perform classification, though we had to assign to each window a unique label, which we decided to be corresponding to the label present with more samples. This doesn't constitute a problem per se, even when changing the size of the sliding window, as long as it is kept short enough and ...


\section{Learning Framework}
\label{sec:learning_framework}


One of the main problems in Human Activity Recognition is handling \text{inactivity}.

Thinking of a real recognition system, 
In this paper we compare two different learning strategies, mimicking a real system. In the first \ref{sub:oneshot}, \text{One Shot Classification}, the model is trained to learn a representation of the involved classes together with the null class

\subsection{One Shot Classification}
\label{sub:oneshot}

\subsection{Two Steps Classification}
\label{sub:twosteps}
