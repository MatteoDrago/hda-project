{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scipy.io.loadmat(\"data_temp/S1-ADL1\", mdict={'features':'features', 'labels':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  87.,  975., -287., ...,   20.,   42.,  175.],\n",
       "       [ 124.,  978., -389., ...,   17.,   31.,  175.],\n",
       "       [ 102.,  996., -440., ...,  -27.,   15.,  175.],\n",
       "       ...,\n",
       "       [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
       "       [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
       "       [  nan,   nan,   nan, ...,   nan,   nan,   nan]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column indexes of signals to be kept\n",
    "idx_signals = list(np.arange(1,46)) \\\n",
    "            + list(np.arange(50,59)) \\\n",
    "            + list(np.arange(63,72)) \\\n",
    "            + list(np.arange(76,85)) \\\n",
    "            + list(np.arange(89,98)) \\\n",
    "            + list(np.arange(102,134))\n",
    "# column indexes of labels\n",
    "idx_labels = list(np.arange(243,250))\n",
    "# import dataset\n",
    "features_all = pd.read_table('./OpportunityUCIDataset/dataset/S1-ADL1.dat', sep=\"\\s+\", header=None, usecols=idx_signals)\n",
    "labels_all = pd.read_table('./OpportunityUCIDataset/dataset/S1-ADL1.dat', sep=\"\\s+\", header=None, usecols=idx_labels)\n",
    "\n",
    "print('\\nImported data:\\n\\n', features_all.head())\n",
    "print('\\nImported data:\\n\\n', labels_all.head())\n",
    "\n",
    "# interpolation\n",
    "# os.system(\"interpolation.py\")\n",
    "\n",
    "\n",
    "# CLASSIFICATION\n",
    "\n",
    "# parameters\n",
    "batch_size, seq_length, n_channels = 1, 50, 113\n",
    "stride = 25\n",
    "activity_label = 1\n",
    "labels = labels_all.iloc[:,activity_label]\n",
    "\n",
    "print(\"\\nBatch size: \", batch_size, \"\\nSequence length: \", seq_length)\n",
    "print(\"\\nLabels:\\n\", labels.head())\n",
    "\n",
    "# placeholders\n",
    "X = tf.placeholder(tf.float32, shape=[None, seq_length, n_channels], name='input')\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name='label')\n",
    "\n",
    "# layers\n",
    "conv_1 = tf.layers.conv1d(inputs=X, filters=64, kernel_size=2, activation=tf.nn.relu)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv_1, pool_size=2, strides=2, padding='same')\n",
    "dropout_1 = tf.layers.dropout(inputs=max_pool_1, rate=0.3)\n",
    "\n",
    "conv_2 = tf.layers.conv1d(inputs=dropout_1, filters=36, kernel_size=1, activation=tf.nn.relu)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv_2, pool_size=2, strides=2, padding='same')\n",
    "dropout_2 = tf.layers.dropout(inputs=max_pool_2, rate=0.3)\n",
    "\n",
    "full_1 = tf.layers.dense(inputs=dropout_2, units=10)\n",
    "\n",
    "y_pred = tf.layers.dense(inputs=full_1, units=4)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_pred))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# session\n",
    "def next_batch(step, seq_length, batch_size):\n",
    "    idx_from = step * stride\n",
    "    batch_x = features_all[idx_from:idx_from+seq_length]\n",
    "    batch_y = labels_all[idx_from:idx_from+seq_length]\n",
    "    # use histogram to select a unique lable\n",
    "    batch_y = batch_y[1,1]\n",
    "\n",
    "    return batch_x, batch_y\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "steps = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        \n",
    "        batch_x, batch_y = next_batch(i, seq_length, batch_size)\n",
    "        \n",
    "        sess.run(train, feed_dict={X:batch_x, y:batch_y})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "\n",
    "            print(sess.run(acc,feed_dict={X:batch_x, y:batch_y}))\n",
    "            print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
