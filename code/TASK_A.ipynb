{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3: TASK A\n",
    "## Classification of Modes of Locomotion\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: column of features to be selected to perform activity detection, between [0,6];\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters definition\n",
    "\n",
    "subject = 1\n",
    "folder = \"./data/reduced/\"\n",
    "label_col = 0     # default for task A\n",
    "window_size = 15\n",
    "stride = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Loading and Preprocessing\n",
    "\n",
    "### Dataset Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session shapes:\n",
      "ADL1:   (45810, 58)\n",
      "ADL2:   (28996, 58)\n",
      "ADL3:   (30167, 58)\n",
      "ADL4:   (30228, 58)\n",
      "ADL5:   (27308, 58)\n",
      "Drill:  (52152, 58)\n",
      "\n",
      "Training samples:  157125 \n",
      "Test samples:       57536 \n",
      "Features:             58\n"
     ]
    }
   ],
   "source": [
    "# import all sessions for a subject\n",
    "(data1, data2, data3, data4, data5, data6) = utils.loadData(subject, folder=folder)\n",
    "\n",
    "# create training set and test set\n",
    "X_train = np.concatenate((data1['features_interp'],\\\n",
    "                          data2['features_interp'],\\\n",
    "                          data3['features_interp'],\\\n",
    "                          data6['features_interp']), axis=0)\n",
    "\n",
    "Y_train = np.concatenate((data1['labels_cut'][:,label_col],\\\n",
    "                          data2['labels_cut'][:,label_col],\\\n",
    "                          data3['labels_cut'][:,label_col],\\\n",
    "                          data6['labels_cut'][:,label_col]), axis=0)\n",
    "\n",
    "X_test = np.concatenate((data4['features_interp'],\\\n",
    "                         data5['features_interp']), axis=0)\n",
    "\n",
    "Y_test = np.concatenate((data4['labels_cut'][:,label_col],\\\n",
    "                         data5['labels_cut'][:,label_col]))\n",
    "\n",
    "features = X_test.shape[1]\n",
    "print(\"\\nTraining samples: \", X_train.shape[0],\\\n",
    "      \"\\nTest samples:      \", X_test.shape[0],\\\n",
    "      \"\\nFeatures:            \", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation in order to feed it to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes in training set:  5 \n",
      "Classes in test set:      5\n",
      "Training set:\n",
      "<class 'numpy.ndarray'> (10474, 15, 58) <class 'numpy.ndarray'> (10474, 5)\n",
      "\n",
      "Features have shape:  (10474, 15, 58) \n",
      "Labels have shape:    (10474, 5) \n",
      "Fraction of labels:   [0.11075043 0.41789192 0.27592133 0.17099484 0.02444147]\n",
      "\n",
      "Test set:\n",
      "<class 'numpy.ndarray'> (3834, 15, 58) <class 'numpy.ndarray'> (3834, 5)\n",
      "\n",
      "Features have shape:  (3834, 15, 58) \n",
      "Labels have shape:    (3834, 5) \n",
      "Fraction of labels:   [0.17736046 0.342723   0.20396453 0.23761085 0.03834116]\n"
     ]
    }
   ],
   "source": [
    "# decision to overcome the problem of entire missing columns\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "# features normalization\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train =scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# switch to one hot encoded labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "Y_train_oh = onehot_encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "Y_test_oh = onehot_encoder.fit_transform(Y_test.reshape(-1, 1))\n",
    "print(\"\\nClasses in training set: \", Y_train_oh.shape[1],\\\n",
    "      \"\\nClasses in test set:     \", Y_test_oh.shape[1])\n",
    "\n",
    "print(\"Training set:\")\n",
    "X_train_s, Y_train_s = utils.prepareData(X_train, Y_train_oh, window_size, stride, shuffle=False)\n",
    "print(\"\\nTest set:\")\n",
    "X_test_s, Y_test_s = utils.prepareData(X_test, Y_test_oh, window_size, stride, shuffle=False)\n",
    "# add bars plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 - Classification with Conv1D\n",
    "\n",
    "### Creation of one-dimensional convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15, 18)            5238      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 18)            72        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 15, 18)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 18)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 36)             4572      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 36)             144       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 36)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 36)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 36)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 4, 72)             18216     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 72)             288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 72)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 72)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                9280      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 38,135\n",
      "Trainable params: 37,883\n",
      "Non-trainable params: 252\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classes = np.max((Y_train_s.shape[1], Y_test_s.shape[1]))\n",
    "\n",
    "model_unidim = utils.Model1D((window_size, features), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01)\n",
    "model_unidim.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10474 samples, validate on 3834 samples\n",
      "Epoch 1/10\n",
      "10474/10474 [==============================] - 4s 384us/step - loss: 0.9330 - acc: 0.7616 - val_loss: 0.5495 - val_acc: 0.8503\n",
      "Epoch 2/10\n",
      "10474/10474 [==============================] - 1s 98us/step - loss: 0.5245 - acc: 0.8245 - val_loss: 0.5243 - val_acc: 0.8633\n",
      "Epoch 3/10\n",
      "10474/10474 [==============================] - 1s 99us/step - loss: 0.4810 - acc: 0.8333 - val_loss: 0.5533 - val_acc: 0.8589\n",
      "Epoch 4/10\n",
      "10474/10474 [==============================] - 1s 99us/step - loss: 0.4500 - acc: 0.8471 - val_loss: 0.5195 - val_acc: 0.8644\n",
      "Epoch 5/10\n",
      "10474/10474 [==============================] - 1s 98us/step - loss: 0.4414 - acc: 0.8527 - val_loss: 0.5012 - val_acc: 0.8735\n",
      "Epoch 6/10\n",
      "10474/10474 [==============================] - 1s 98us/step - loss: 0.4310 - acc: 0.8558 - val_loss: 0.5491 - val_acc: 0.8693\n",
      "Epoch 7/10\n",
      "10474/10474 [==============================] - 1s 99us/step - loss: 0.4125 - acc: 0.8625 - val_loss: 0.5236 - val_acc: 0.8748\n",
      "Epoch 8/10\n",
      "10474/10474 [==============================] - 1s 98us/step - loss: 0.4015 - acc: 0.8660 - val_loss: 0.4935 - val_acc: 0.8818\n",
      "Epoch 9/10\n",
      "10474/10474 [==============================] - 1s 99us/step - loss: 0.3946 - acc: 0.8685 - val_loss: 0.4846 - val_acc: 0.8732\n",
      "Epoch 10/10\n",
      "10474/10474 [==============================] - 1s 99us/step - loss: 0.3889 - acc: 0.8725 - val_loss: 0.5429 - val_acc: 0.8685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18f1f933668>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_unidim.fit(x = X_train_s, y = Y_train_s, epochs = 10, batch_size = 128, validation_data=(X_test_s, Y_test_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and evaluation of performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels\n",
    "Y_pred_s = model_unidim.predict(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "#reverse the one-ot encoder procedure\n",
    "Y_test_hard = np.argmax(Y_test_s, axis=1)\n",
    "Y_pred_hard = np.argmax(Y_pred_s, axis=1)\n",
    "\n",
    "print(\"F1-measure: \", utils.f1_score(Y_test_hard, Y_pred_hard, average='weighted'))\n",
    "print(\"AUC w.r. to each class: \", utils.AUC(Y_test_s, Y_pred_s, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot confusion matrix\n",
    "cnf_matrix = utils.confusion_matrix(Y_test_hard, Y_pred_hard)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=[0,1,2,4,5],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 - Classification with Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new2D = utils.Model2D((1,window_size,features), classes)\n",
    "opt4 = Adam(lr=0.01)\n",
    "model_new2D.compile(optimizer = opt4, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reshape in order to fit to the new 2D model\n",
    "X_train = X_train_s.reshape(X_train_s.shape[0], 1, window_size, features)\n",
    "X_test = X_test_s.reshape(X_test_s.shape[0], 1, window_size, features)\n",
    "\n",
    "model_new2D.fit(x = X_train, y = Y_train_s, epochs = 10, batch_size = 128, validation_data=(X_test, Y_test_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and evaluation of performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels\n",
    "Y_pred_s = model_new2D.predict(X_test)\n",
    "\n",
    "# print results\n",
    "#reverse the one-ot encoder procedure\n",
    "Y_test_hard = np.argmax(Y_test_s, axis=1)\n",
    "Y_pred_hard = np.argmax(Y_pred_s, axis=1)\n",
    "\n",
    "print(\"F1-measure: \", utils.f1_score(Y_test_hard, Y_pred_hard, average='weighted'))\n",
    "print(\"AUC w.r. to each class: \", utils.AUC(Y_test_s, Y_pred_s, classes))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cnf_matrix = utils.confusion_matrix(Y_test_hard, Y_pred_hard)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=[0,1,2,4,5],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
