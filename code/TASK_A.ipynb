{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3: TASK A\n",
    "## Classification of Modes of Locomotion\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: column of features to be selected to perform activity detection, between [0,6];\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 2\n",
    "folder = \"./data/full/\"\n",
    "trim_zeros = True\n",
    "label_col = 0     # default for task A\n",
    "window_size = 15\n",
    "stride = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session shapes:\n",
      "ADL1:   (38733, 110)\n",
      "ADL2:   (26824, 110)\n",
      "ADL3:   (31242, 110)\n",
      "ADL4:   (29723, 110)\n",
      "ADL5:   (27997, 110)\n",
      "Drill:  (49009, 110)\n",
      "\n",
      "Training samples:  131959 \n",
      "Test samples:       48486 \n",
      "Features:             110\n"
     ]
    }
   ],
   "source": [
    "# import all sessions for a subject\n",
    "(data1, data2, data3, data4, data5, data6) = utils.loadData(subject, folder=folder)\n",
    "\n",
    "# create training set and test set\n",
    "X_train = np.concatenate((data1['features'],\\\n",
    "                          data2['features'],\\\n",
    "                          data3['features'],\\\n",
    "                          data6['features']), axis=0)\n",
    "\n",
    "Y_train = np.concatenate((data1['labels'][:,label_col],\\\n",
    "                          data2['labels'][:,label_col],\\\n",
    "                          data3['labels'][:,label_col],\\\n",
    "                          data6['labels'][:,label_col]), axis=0)\n",
    "\n",
    "X_test = np.concatenate((data4['features'],\\\n",
    "                         data5['features']), axis=0)\n",
    "\n",
    "Y_test = np.concatenate((data4['labels'][:,label_col],\\\n",
    "                         data5['labels'][:,label_col]))\n",
    "\n",
    "features = X_test.shape[1]\n",
    "\n",
    "if trim_zeros:\n",
    "    mask = np.where(Y_train == 0)[0]\n",
    "    Y_train = np.delete(Y_train,mask)\n",
    "    X_train = np.delete(X_train,mask,axis=0)\n",
    "\n",
    "    mask = np.where(Y_test == 0)[0]\n",
    "    Y_test = np.delete(Y_test,mask)\n",
    "    X_test = np.delete(X_test,mask,axis=0)\n",
    "\n",
    "print(\"\\nTraining samples: \", X_train.shape[0],\\\n",
    "      \"\\nTest samples:      \", X_test.shape[0],\\\n",
    "      \"\\nFeatures:            \", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes in training set:  4 \n",
      "Classes in test set:      4\n"
     ]
    }
   ],
   "source": [
    "# decision to overcome the problem of entire missing columns\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "# features normalization\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# switch to one hot encoded labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categorical_features='all')\n",
    "\n",
    "Y_train_oh = onehot_encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "Y_test_oh = onehot_encoder.fit_transform(Y_test.reshape(-1, 1))\n",
    "print(\"\\nClasses in training set: \", Y_train_oh.shape[1],\\\n",
    "      \"\\nClasses in test set:     \", Y_test_oh.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of data in a input-suitable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features have shape:  (8796, 15, 110) \n",
      "Labels have shape:    (8796, 4) \n",
      "Fraction of labels:   [0.46737153 0.29115507 0.2186221  0.0228513 ]\n",
      "\n",
      "Features have shape:  (3231, 15, 110) \n",
      "Labels have shape:    (3231, 4) \n",
      "Fraction of labels:   [0.36892603 0.28071804 0.31104921 0.03930672]\n"
     ]
    }
   ],
   "source": [
    "X_train_s, Y_train_s = utils.prepareData(X_train, Y_train_oh, window_size, stride, shuffle=False)\n",
    "X_test_s, Y_test_s = utils.prepareData(X_test, Y_test_oh, window_size, stride, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.max((Y_train_oh.shape[1], Y_test_oh.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Performances\n",
    "\n",
    "## 1D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 15, 18)            9918      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15, 18)            72        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 15, 18)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8, 18)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8, 36)             4572      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 36)             144       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 8, 36)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 4, 36)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 36)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 4, 72)             18216     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 72)             288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 72)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 2, 72)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                9280      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 42,750\n",
      "Trainable params: 42,498\n",
      "Non-trainable params: 252\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_unidim = utils.Model1D((window_size, features), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8796 samples, validate on 3231 samples\n",
      "Epoch 1/15\n",
      "8796/8796 [==============================] - 3s 380us/step - loss: 1.0444 - acc: 0.7666 - val_loss: 0.6852 - val_acc: 0.7252\n",
      "Epoch 2/15\n",
      "8796/8796 [==============================] - 2s 257us/step - loss: 0.5879 - acc: 0.8273 - val_loss: 0.4549 - val_acc: 0.8706\n",
      "Epoch 3/15\n",
      "8796/8796 [==============================] - 2s 255us/step - loss: 0.4909 - acc: 0.8424 - val_loss: 0.7884 - val_acc: 0.7707\n",
      "Epoch 4/15\n",
      "8796/8796 [==============================] - 2s 258us/step - loss: 0.4600 - acc: 0.8438 - val_loss: 0.7829 - val_acc: 0.7639\n",
      "Epoch 5/15\n",
      "8796/8796 [==============================] - 2s 263us/step - loss: 0.4241 - acc: 0.8556 - val_loss: 1.1970 - val_acc: 0.7753\n",
      "Epoch 6/15\n",
      "8796/8796 [==============================] - 2s 260us/step - loss: 0.4147 - acc: 0.8620 - val_loss: 0.7566 - val_acc: 0.7762\n",
      "Epoch 7/15\n",
      "8796/8796 [==============================] - 2s 255us/step - loss: 0.4020 - acc: 0.8636 - val_loss: 0.9971 - val_acc: 0.7809\n",
      "Epoch 8/15\n",
      "8796/8796 [==============================] - 2s 273us/step - loss: 0.4166 - acc: 0.8619 - val_loss: 0.6168 - val_acc: 0.7688\n",
      "Epoch 9/15\n",
      "8796/8796 [==============================] - 3s 287us/step - loss: 0.4065 - acc: 0.8678 - val_loss: 0.6296 - val_acc: 0.7818\n",
      "Epoch 10/15\n",
      "8796/8796 [==============================] - 2s 260us/step - loss: 0.3869 - acc: 0.8663 - val_loss: 0.5910 - val_acc: 0.7700\n",
      "Epoch 11/15\n",
      "8796/8796 [==============================] - 2s 267us/step - loss: 0.3821 - acc: 0.8712 - val_loss: 0.9157 - val_acc: 0.7611\n",
      "Epoch 12/15\n",
      "8796/8796 [==============================] - 2s 259us/step - loss: 0.3682 - acc: 0.8745 - val_loss: 0.4454 - val_acc: 0.7908\n",
      "Epoch 13/15\n",
      "8796/8796 [==============================] - 2s 260us/step - loss: 0.3673 - acc: 0.8738 - val_loss: 1.3675 - val_acc: 0.7790\n",
      "Epoch 14/15\n",
      "8796/8796 [==============================] - 2s 263us/step - loss: 0.3795 - acc: 0.8747 - val_loss: 1.0542 - val_acc: 0.7753\n",
      "Epoch 15/15\n",
      "8796/8796 [==============================] - 2s 261us/step - loss: 0.4325 - acc: 0.8716 - val_loss: 0.8158 - val_acc: 0.7762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1362c178fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Adam(lr=0.01)\n",
    "model_unidim.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model_unidim.fit(x = X_train_s, y = Y_train_s, epochs = 15, batch_size = 128, validation_data=(X_test_s, Y_test_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels\n",
    "Y_pred_s = model_unidim.predict(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "#reverse the one-hot encoder procedure\n",
    "Y_test_hard = np.argmax(Y_test_s, axis=1)\n",
    "Y_pred_hard = np.argmax(Y_pred_s, axis=1)\n",
    "\n",
    "print(\"F1-measure: \", utils.f1_score(Y_test_hard, Y_pred_hard, average='weighted'))\n",
    "print(\"AUC w.r. to each class: \", utils.AUC(Y_test_s, Y_pred_s, classes))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cnf_matrix = utils.confusion_matrix(Y_test_hard, Y_pred_hard)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix,classes = [1,2,4,5],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
