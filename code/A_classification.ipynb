{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import deeplearning\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.activations import relu\n",
    "from keras.layers import Conv2D, BatchNormalization, Dropout, LeakyReLU, Flatten, Activation, Dense, MaxPooling2D, LSTM, Reshape\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the hyper-parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: column of features to be selected to perform activity detection, between [0,6]:\n",
    "\n",
    "|  Label |  Feature |\n",
    "|:-:     |:-:|\n",
    "|  0     | Locomotion (TASK A)  |\n",
    "|  1     | High Level Activity |\n",
    "|  2     | Low Level Left Arm  |\n",
    "|  3     | Low Level Left Arm Object  |\n",
    "|  4     | Low Level Right Arm  |\n",
    "|  5     | Low Level Right Arm Object  |\n",
    "|  6     | Medium Level Both Arms (TASK B2) |\n",
    "\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window.\n",
    "\n",
    "The size of the temporal window seems to be fundamental in order to get a more specific and powerful model; of course the choice of the step lenght between consequent windows has to be consistent and to make sense. Thinking about a real-time situation, as long as we collect data we can use a sliding window of real-time samples; in this way, it is reasonable to use also a small value for the stride. Another important reason behind the choice of the value of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [1,2,3,4]\n",
    "folder = \"./data/full/\"\n",
    "#folder = \"/floyd/input/hdadataset/full/\" # To be used with FloydHub\n",
    "label = 0     # default for task A\n",
    "window_size = 64\n",
    "stride = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "After the _detection_ step, this time we exclude all the samples associated to the _null class_; in this way we can build a neural network cleaned of the null class and that can distinguish better the difference between motions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition, compilation and input reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 110 #number of features taken into consideration for the solution of the problem\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 64, 110, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 54, 110, 50)       600       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 110, 50)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 27, 5500)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 27, 300)           6961200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 7,839,168\n",
      "Trainable params: 7,839,166\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model = deeplearning.MotionClassification((window_size,n_features,1), n_classes)\n",
    "classification_model.summary() # model visualization\n",
    "\n",
    "classification_model.compile(optimizer = Adam(lr=0.01), \n",
    "                   loss = \"categorical_crossentropy\", \n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "\n",
    "After the training procedure, the model will be saved on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going for USER  1\n",
      "Training samples:  157125 \n",
      "Test samples:       57536 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (46601, 64, 110) \n",
      "Dataset of Labels have shape:    (46601, 4) \n",
      "Fraction of labels:   [0.47170662 0.30868436 0.19233493 0.02727409]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (15758, 64, 110) \n",
      "Dataset of Labels have shape:    (15758, 4) \n",
      "Fraction of labels:   [0.41743876 0.24666836 0.28899607 0.04689681]\n",
      "Train on 46601 samples, validate on 15758 samples\n",
      "Epoch 1/20\n",
      "46601/46601 [==============================] - 147s 3ms/step - loss: 0.4000 - acc: 0.8626 - val_loss: 0.2739 - val_acc: 0.9200\n",
      "Epoch 2/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2020 - acc: 0.9221 - val_loss: 0.1966 - val_acc: 0.9334\n",
      "Epoch 3/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2036 - acc: 0.9224 - val_loss: 0.2730 - val_acc: 0.9278\n",
      "Epoch 4/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.1785 - acc: 0.9317 - val_loss: 0.2274 - val_acc: 0.9379\n",
      "Epoch 5/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.1835 - acc: 0.9301 - val_loss: 0.1680 - val_acc: 0.9456\n",
      "Epoch 6/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.1968 - acc: 0.9258 - val_loss: 0.2916 - val_acc: 0.9255\n",
      "Epoch 7/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2750 - acc: 0.8938 - val_loss: 0.3108 - val_acc: 0.8906\n",
      "Epoch 8/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2854 - acc: 0.8893 - val_loss: 0.2740 - val_acc: 0.9251\n",
      "Epoch 9/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2259 - acc: 0.9149 - val_loss: 0.1676 - val_acc: 0.9331\n",
      "Epoch 10/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2244 - acc: 0.9142 - val_loss: 0.1824 - val_acc: 0.9236\n",
      "Epoch 11/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2123 - acc: 0.9185 - val_loss: 0.2554 - val_acc: 0.9342\n",
      "Epoch 12/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2152 - acc: 0.9190 - val_loss: 0.2806 - val_acc: 0.9239\n",
      "Epoch 13/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2027 - acc: 0.9220 - val_loss: 0.1853 - val_acc: 0.9360\n",
      "Epoch 14/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2015 - acc: 0.9243 - val_loss: 0.2178 - val_acc: 0.9423\n",
      "Epoch 15/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2033 - acc: 0.9218 - val_loss: 0.1814 - val_acc: 0.9265\n",
      "Epoch 16/20\n",
      "46601/46601 [==============================] - 142s 3ms/step - loss: 0.2146 - acc: 0.9204 - val_loss: 0.1748 - val_acc: 0.9335\n",
      "Epoch 17/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.1964 - acc: 0.9252 - val_loss: 0.2119 - val_acc: 0.9308\n",
      "Epoch 18/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.1971 - acc: 0.9258 - val_loss: 0.1524 - val_acc: 0.9403\n",
      "Epoch 19/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2143 - acc: 0.9165 - val_loss: 0.1946 - val_acc: 0.9397\n",
      "Epoch 20/20\n",
      "46601/46601 [==============================] - 141s 3ms/step - loss: 0.2136 - acc: 0.9202 - val_loss: 0.1690 - val_acc: 0.9367\n",
      "Going for USER  2\n",
      "Training samples:  145808 \n",
      "Test samples:       57720 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (44182, 64, 110) \n",
      "Dataset of Labels have shape:    (44182, 4) \n",
      "Fraction of labels:   [0.46335612 0.29527862 0.21861844 0.02274682]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (16145, 64, 110) \n",
      "Dataset of Labels have shape:    (16145, 4) \n",
      "Fraction of labels:   [0.36599566 0.28436048 0.31062248 0.03902137]\n",
      "Train on 44182 samples, validate on 16145 samples\n",
      "Epoch 1/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.6917 - acc: 0.7592 - val_loss: 0.7695 - val_acc: 0.7186\n",
      "Epoch 2/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.4926 - acc: 0.8182 - val_loss: 0.7501 - val_acc: 0.7002\n",
      "Epoch 3/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.4701 - acc: 0.8265 - val_loss: 0.7959 - val_acc: 0.7563\n",
      "Epoch 4/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.4321 - acc: 0.8413 - val_loss: 0.8209 - val_acc: 0.7420\n",
      "Epoch 5/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.4086 - acc: 0.8497 - val_loss: 1.1389 - val_acc: 0.7402\n",
      "Epoch 6/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3837 - acc: 0.8588 - val_loss: 0.9247 - val_acc: 0.7497\n",
      "Epoch 7/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3753 - acc: 0.8640 - val_loss: 0.7571 - val_acc: 0.7487\n",
      "Epoch 8/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3809 - acc: 0.8626 - val_loss: 0.9464 - val_acc: 0.7070\n",
      "Epoch 9/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3612 - acc: 0.8689 - val_loss: 0.7790 - val_acc: 0.7911\n",
      "Epoch 10/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3786 - acc: 0.8626 - val_loss: 0.8766 - val_acc: 0.7475\n",
      "Epoch 11/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3878 - acc: 0.8552 - val_loss: 0.7503 - val_acc: 0.7482\n",
      "Epoch 12/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3913 - acc: 0.8512 - val_loss: 1.0015 - val_acc: 0.7555\n",
      "Epoch 13/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3862 - acc: 0.8579 - val_loss: 0.9032 - val_acc: 0.7591\n",
      "Epoch 14/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3861 - acc: 0.8566 - val_loss: 0.8827 - val_acc: 0.7480\n",
      "Epoch 15/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.4080 - acc: 0.8515 - val_loss: 0.8337 - val_acc: 0.7880\n",
      "Epoch 16/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3877 - acc: 0.8576 - val_loss: 1.0816 - val_acc: 0.7848\n",
      "Epoch 17/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3800 - acc: 0.8554 - val_loss: 0.9493 - val_acc: 0.7921\n",
      "Epoch 18/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3670 - acc: 0.8661 - val_loss: 1.0978 - val_acc: 0.7490\n",
      "Epoch 19/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3541 - acc: 0.8692 - val_loss: 0.3955 - val_acc: 0.7513\n",
      "Epoch 20/20\n",
      "44182/44182 [==============================] - 135s 3ms/step - loss: 0.3576 - acc: 0.8684 - val_loss: 0.9702 - val_acc: 0.7880\n",
      "Going for USER  3\n",
      "Training samples:  150098 \n",
      "Test samples:       49005 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (48988, 64, 110) \n",
      "Dataset of Labels have shape:    (48988, 4) \n",
      "Fraction of labels:   [0.5775496  0.25295991 0.13227729 0.0372132 ]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (15254, 64, 110) \n",
      "Dataset of Labels have shape:    (15254, 4) \n",
      "Fraction of labels:   [0.42696998 0.29218566 0.20086535 0.07997902]\n",
      "Train on 48988 samples, validate on 15254 samples\n",
      "Epoch 1/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.7621 - acc: 0.7130 - val_loss: 0.6826 - val_acc: 0.7643\n",
      "Epoch 2/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.5579 - acc: 0.7918 - val_loss: 0.4747 - val_acc: 0.8444\n",
      "Epoch 3/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.5065 - acc: 0.8234 - val_loss: 0.4825 - val_acc: 0.8467\n",
      "Epoch 4/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.4249 - acc: 0.8560 - val_loss: 0.3666 - val_acc: 0.8712\n",
      "Epoch 5/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3942 - acc: 0.8676 - val_loss: 0.3779 - val_acc: 0.8760\n",
      "Epoch 6/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3783 - acc: 0.8722 - val_loss: 0.4159 - val_acc: 0.8579\n",
      "Epoch 7/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.4006 - acc: 0.8663 - val_loss: 0.4302 - val_acc: 0.8489\n",
      "Epoch 8/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3702 - acc: 0.8773 - val_loss: 0.3511 - val_acc: 0.8661\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3812 - acc: 0.8647 - val_loss: 0.4961 - val_acc: 0.8581\n",
      "Epoch 10/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3677 - acc: 0.8715 - val_loss: 0.3788 - val_acc: 0.8662\n",
      "Epoch 11/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3594 - acc: 0.8753 - val_loss: 0.5242 - val_acc: 0.8649\n",
      "Epoch 12/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3276 - acc: 0.8855 - val_loss: 0.3422 - val_acc: 0.8757\n",
      "Epoch 13/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3065 - acc: 0.8901 - val_loss: 0.3176 - val_acc: 0.8876\n",
      "Epoch 14/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3278 - acc: 0.8828 - val_loss: 0.3843 - val_acc: 0.8539\n",
      "Epoch 15/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3191 - acc: 0.8903 - val_loss: 0.5742 - val_acc: 0.8206\n",
      "Epoch 16/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3228 - acc: 0.8887 - val_loss: 0.4474 - val_acc: 0.8473\n",
      "Epoch 17/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3240 - acc: 0.8845 - val_loss: 0.3318 - val_acc: 0.8726\n",
      "Epoch 18/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3016 - acc: 0.8953 - val_loss: 0.3419 - val_acc: 0.8724\n",
      "Epoch 19/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.2986 - acc: 0.8932 - val_loss: 0.3534 - val_acc: 0.8572\n",
      "Epoch 20/20\n",
      "48988/48988 [==============================] - 147s 3ms/step - loss: 0.3108 - acc: 0.8861 - val_loss: 0.3705 - val_acc: 0.8509\n",
      "Going for USER  4\n",
      "Training samples:  118493 \n",
      "Test samples:       45675 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (36042, 64, 110) \n",
      "Dataset of Labels have shape:    (36042, 4) \n",
      "Fraction of labels:   [0.55102381 0.25009711 0.16564009 0.033239  ]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (12956, 64, 110) \n",
      "Dataset of Labels have shape:    (12956, 4) \n",
      "Fraction of labels:   [0.44025934 0.36477308 0.15074097 0.04422661]\n",
      "Train on 36042 samples, validate on 12956 samples\n",
      "Epoch 1/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.4758 - acc: 0.8459 - val_loss: 0.4327 - val_acc: 0.8424\n",
      "Epoch 2/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.4377 - acc: 0.8546 - val_loss: 0.3604 - val_acc: 0.8841\n",
      "Epoch 3/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.4031 - acc: 0.8652 - val_loss: 0.3590 - val_acc: 0.8738\n",
      "Epoch 4/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3772 - acc: 0.8762 - val_loss: 0.3523 - val_acc: 0.8811\n",
      "Epoch 5/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3690 - acc: 0.8773 - val_loss: 0.3316 - val_acc: 0.8792\n",
      "Epoch 6/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3885 - acc: 0.8678 - val_loss: 0.3318 - val_acc: 0.8716\n",
      "Epoch 7/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3677 - acc: 0.8746 - val_loss: 0.3101 - val_acc: 0.8897\n",
      "Epoch 8/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3705 - acc: 0.8762 - val_loss: 0.2898 - val_acc: 0.8995\n",
      "Epoch 9/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3538 - acc: 0.8803 - val_loss: 0.3679 - val_acc: 0.8743\n",
      "Epoch 10/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3452 - acc: 0.8835 - val_loss: 0.2909 - val_acc: 0.8940\n",
      "Epoch 11/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3283 - acc: 0.8866 - val_loss: 0.3092 - val_acc: 0.8774\n",
      "Epoch 12/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3307 - acc: 0.8863 - val_loss: 0.2981 - val_acc: 0.8814\n",
      "Epoch 13/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3404 - acc: 0.8828 - val_loss: 0.3192 - val_acc: 0.8769\n",
      "Epoch 14/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3562 - acc: 0.8778 - val_loss: 0.3359 - val_acc: 0.8558\n",
      "Epoch 15/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3372 - acc: 0.8842 - val_loss: 0.3115 - val_acc: 0.8756\n",
      "Epoch 16/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3368 - acc: 0.8831 - val_loss: 0.2924 - val_acc: 0.8830\n",
      "Epoch 17/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3267 - acc: 0.8851 - val_loss: 0.3305 - val_acc: 0.8662\n",
      "Epoch 18/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3150 - acc: 0.8933 - val_loss: 0.3376 - val_acc: 0.8552\n",
      "Epoch 19/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3406 - acc: 0.8789 - val_loss: 0.3098 - val_acc: 0.8730\n",
      "Epoch 20/20\n",
      "36042/36042 [==============================] - 110s 3ms/step - loss: 0.3356 - acc: 0.8806 - val_loss: 0.3277 - val_acc: 0.8793\n"
     ]
    }
   ],
   "source": [
    "for s in subjects:\n",
    "    \n",
    "    print(\"Going for USER \", s)\n",
    "    \n",
    "    [x_train, y_train, x_test, y_test, n_classes] = utils.preprocessing(s,\n",
    "                                                                    folder,\n",
    "                                                                    label,\n",
    "                                                                    window_size,\n",
    "                                                                    stride,\n",
    "                                                                    null_class = False)\n",
    "    \n",
    "    input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "    input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1)\n",
    "    \n",
    "    classification_model.fit(x = input_train, \n",
    "                   y = y_train, \n",
    "                   epochs = 20, \n",
    "                   batch_size = 300,\n",
    "                   verbose = 1,\n",
    "                   validation_data=(input_test, y_test))\n",
    "\n",
    "classification_model.save('./data/classification_model_A.h5')\n",
    "classification_model.save_weights('./data/classification_model_weights_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_model = load_model('./data/classification_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
