{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import deeplearning\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.activations import relu\n",
    "from keras.layers import Conv2D, BatchNormalization, Dropout, LeakyReLU, Flatten, Activation, Dense, MaxPooling2D, LSTM, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been written with the purpose of training the neural network (the one that we want) on the complete set of users. Due to the huge amount of data, even for a single user, I fear that we must divide training and classification section (and import distinctly two different times the dataset for each user). I searched the internet for a suitable solution concerning the consecutive creation of new variables at each iteration of the loop cycle, but none was suitable (as I said, I fear that using a dictionary would overflow the memory). \n",
    "\n",
    "My idea was to study the difference between the performance when we train the network only on the user of interest, and when we instead use the network trained on all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [1,2,3,4]\n",
    "sbj = 1\n",
    "folder = \"./data/full/\"\n",
    "#folder = \"/floyd/input/hdadataset/full/\" # To be used with FloydHub\n",
    "\n",
    "label = 0     # default for task A\n",
    "window_size = 64\n",
    "stride = 3\n",
    "null_class = True\n",
    "epochs = 25\n",
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network definition\n",
    "\n",
    "n_features = 110 #number of features taken into consideration for the solution of the problem\n",
    "\n",
    "model = deeplearning.Hybrid((window_size,n_features,1), n_classes)\n",
    "model.summary() # model visualization\n",
    "\n",
    "model.compile(optimizer = Adam(lr=0.01), \n",
    "                   loss = \"categorical_crossentropy\", \n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "for sbj in subjects:\n",
    "    \n",
    "    print('Training for SUBJECT ' % (sbj))\n",
    "    \n",
    "    [x_train, y_train, x_test, y_test, n_classes] = utils.preprocessing(sbj,\n",
    "                                                         folder,\n",
    "                                                         label,\n",
    "                                                         window_size,\n",
    "                                                         stride,\n",
    "                                                         null_class)\n",
    "    \n",
    "    # we need to perform the following operation in order to provide \n",
    "    input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "    input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1) \n",
    "    \n",
    "    model_hyb.fit(x = input_train, \n",
    "               y = y_train, \n",
    "               epochs = epochs, \n",
    "               batch_size = batch_size,\n",
    "               verbose = 1,\n",
    "               validation_data=(input_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION\n",
    "\n",
    "for sbj in subjects:\n",
    "    \n",
    "    print('Classificaion for SUBJECT ' % (sbj))\n",
    "    \n",
    "    [x_train, y_train, x_test, y_test, n_classes] = utils.preprocessing(sbj,\n",
    "                                                         folder,\n",
    "                                                         label,\n",
    "                                                         window_size,\n",
    "                                                         stride,\n",
    "                                                         null_class)\n",
    "    \n",
    "    # we need to perform the following operation in order to provide \n",
    "    input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "    input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1) \n",
    "    \n",
    "    [trainingFeatures, testingFeatures] = deeplearning.extractFeatures(model,\n",
    "                                                                       input_train,\n",
    "                                                                       input_test,\n",
    "                                                                       model.layers[-1].output_shape[1],\n",
    "                                                                       batchSize = 300)\n",
    "    \n",
    "    \n",
    "    prediction_encoded = model.predict(input_test) # prediction on the test set using the trained model \n",
    "    \n",
    "    #For training the Support Vector Machine and for evaluating the model we must switch from the one-hot encoded version to the original one. \n",
    "    output_train = np.argmax(y_train, axis=1)\n",
    "    output_test = np.argmax(y_test, axis=1)\n",
    "    prediction = np.argmax(prediction_encoded, axis=1)\n",
    "\n",
    "    C = [2**(-6)]\n",
    "    prediction_svm = deeplearning.SVMLayer(C,\n",
    "                                           output_train,\n",
    "                                           trainingFeatures,\n",
    "                                           testingFeatures) \n",
    "    print(\"\\nBEFORE SVM:\")\n",
    "    print(\"Accuracy: \", accuracy_score(output_test, prediction))\n",
    "    print(\"F1-measure: \", utils.f1_score(output_test, prediction, average='weighted'))\n",
    "\n",
    "    print(\"\\nAFTER SVM:\")\n",
    "    print(\"Accuracy: \", accuracy_score(output_test, prediction_svm))\n",
    "    print(\"F1-measure: \", utils.f1_score(output_test, prediction_svm, average='weighted'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
