{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B - single subject - model ?\n",
    "\n",
    "## Notebook setup\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- label: index of feature column to be selected to perform activity detection, between [0,6]. The default value for task B is 6;\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 0\n",
    "folder = \"../data/full/\"\n",
    "window_size = 15\n",
    "stride = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "import models\n",
    "import utils\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not(os.path.exists(\"./data\")):\n",
    "    os.mkdir(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot classification\n",
    "Here classification is performed with null class.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data from subject 1\n",
      "\n",
      "Processing data from subject 2\n",
      "\n",
      "Processing data from subject 3\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadDataMultiple(label=label,\n",
    "                                                                                                        folder=folder,\n",
    "                                                                                                        window_size=window_size,\n",
    "                                                                                                        stride=stride,\n",
    "                                                                                                        make_binary=False,\n",
    "                                                                                                        null_class=True,\n",
    "                                                                                                        print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_train and Y_test contain the correct labels for each signals window. Y_test in particular will be used to evaluate predictions for both this (one-shot) and the two-steps models. For this reason it is here saved with a different name, to avoid having it being overwritten later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_true = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot_model = models.Convolutional((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "oneshot_model.compile(optimizer = Adam(lr=0.001),\n",
    "                      loss = \"categorical_crossentropy\", \n",
    "                      metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_AOS_mult.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59176 samples, validate on 21341 samples\n",
      "Epoch 1/15\n",
      "59176/59176 [==============================] - 10s 176us/step - loss: 0.5885 - acc: 0.7851 - val_loss: 0.9146 - val_acc: 0.7602\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.91461, saving model to ./data/model_AOS_mult.hdf5\n",
      "Epoch 2/15\n",
      "59176/59176 [==============================] - 6s 109us/step - loss: 0.4143 - acc: 0.8498 - val_loss: 0.8840 - val_acc: 0.7651\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.91461 to 0.88399, saving model to ./data/model_AOS_mult.hdf5\n",
      "Epoch 3/15\n",
      "59176/59176 [==============================] - 6s 109us/step - loss: 0.3692 - acc: 0.8674 - val_loss: 0.9768 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "59176/59176 [==============================] - 6s 110us/step - loss: 0.3438 - acc: 0.8743 - val_loss: 0.9817 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "59176/59176 [==============================] - 7s 111us/step - loss: 0.3279 - acc: 0.8807 - val_loss: 0.9719 - val_acc: 0.7767\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "59176/59176 [==============================] - 7s 111us/step - loss: 0.3136 - acc: 0.8843 - val_loss: 1.0097 - val_acc: 0.7818\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "59176/59176 [==============================] - 7s 111us/step - loss: 0.3039 - acc: 0.8882 - val_loss: 0.8675 - val_acc: 0.7854\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.88399 to 0.86748, saving model to ./data/model_AOS_mult.hdf5\n",
      "Epoch 8/15\n",
      "59176/59176 [==============================] - 7s 111us/step - loss: 0.2994 - acc: 0.8892 - val_loss: 1.0308 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "59176/59176 [==============================] - 7s 111us/step - loss: 0.2885 - acc: 0.8948 - val_loss: 0.9112 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "59176/59176 [==============================] - 7s 111us/step - loss: 0.2837 - acc: 0.8960 - val_loss: 0.9787 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "59176/59176 [==============================] - 6s 109us/step - loss: 0.2782 - acc: 0.8980 - val_loss: 1.0122 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "59176/59176 [==============================] - 7s 112us/step - loss: 0.2743 - acc: 0.8985 - val_loss: 1.0189 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "59176/59176 [==============================] - 6s 110us/step - loss: 0.2644 - acc: 0.9007 - val_loss: 1.1121 - val_acc: 0.7904\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "59176/59176 [==============================] - 7s 110us/step - loss: 0.2673 - acc: 0.9015 - val_loss: 1.0406 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "59176/59176 [==============================] - 7s 113us/step - loss: 0.2584 - acc: 0.9043 - val_loss: 1.1074 - val_acc: 0.7916\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb01c61438>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneshot_model.fit(x = X_train, \n",
    "                  y = to_categorical(Y_train),\n",
    "                  epochs = 15,\n",
    "                  batch_size = 128,\n",
    "                  verbose = 1,\n",
    "                  callbacks=[checkpointer],\n",
    "                  validation_data=(X_test, to_categorical(Y_test)),\n",
    "                  class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - passare class_weights a class report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.55      0.66      2475\n",
      "          1       0.74      0.93      0.83      7465\n",
      "          2       0.80      0.75      0.77      5445\n",
      "          3       0.86      0.72      0.78      4846\n",
      "          4       0.94      0.92      0.93      1110\n",
      "\n",
      "avg / total       0.80      0.79      0.79     21341\n",
      "\n",
      "Weighted f1-score: 0.7872147643014269\n"
     ]
    }
   ],
   "source": [
    "Y_pred = oneshot_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.60      0.66      2475\n",
      "          1       0.73      0.95      0.83      7465\n",
      "          2       0.82      0.69      0.75      5445\n",
      "          3       0.87      0.71      0.78      4846\n",
      "          4       0.93      0.89      0.91      1110\n",
      "\n",
      "avg / total       0.79      0.79      0.78     21341\n",
      "\n",
      "Weighted f1-score: 0.7813867327367077\n"
     ]
    }
   ],
   "source": [
    "oneshot_model_best = load_model('./data/model_AOS_mult.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-steps classification\n",
    "## Activity detection\n",
    "This model performs a binary classification.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data from subject 1\n",
      "\n",
      "Processing data from subject 2\n",
      "\n",
      "Processing data from subject 3\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadDataMultiple(label=label,\n",
    "                                                                                                        folder=folder,\n",
    "                                                                                                        window_size=window_size,\n",
    "                                                                                                        stride=stride,\n",
    "                                                                                                        make_binary=True,\n",
    "                                                                                                        null_class=True,\n",
    "                                                                                                        print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = models.ConvolutionalRecurrent((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "detection_model.compile(optimizer = Adam(lr=0.001),\n",
    "                        loss = \"categorical_crossentropy\", \n",
    "                        metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_ATSD_mult.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59176 samples, validate on 21341 samples\n",
      "Epoch 1/15\n",
      "59176/59176 [==============================] - 14s 236us/step - loss: 0.1156 - acc: 0.9605 - val_loss: 0.2127 - val_acc: 0.9265\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21267, saving model to ./data/model_ATSD_mult.hdf5\n",
      "Epoch 2/15\n",
      "59176/59176 [==============================] - 11s 178us/step - loss: 0.0805 - acc: 0.9724 - val_loss: 0.2321 - val_acc: 0.9238\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/15\n",
      "59176/59176 [==============================] - 11s 177us/step - loss: 0.0714 - acc: 0.9764 - val_loss: 0.2298 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "59176/59176 [==============================] - 11s 177us/step - loss: 0.0658 - acc: 0.9783 - val_loss: 0.2317 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "59176/59176 [==============================] - 11s 180us/step - loss: 0.0609 - acc: 0.9802 - val_loss: 0.2543 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "59176/59176 [==============================] - 10s 175us/step - loss: 0.0586 - acc: 0.9805 - val_loss: 0.2507 - val_acc: 0.9207\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "59176/59176 [==============================] - 10s 174us/step - loss: 0.0557 - acc: 0.9816 - val_loss: 0.2707 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "59176/59176 [==============================] - 10s 175us/step - loss: 0.0534 - acc: 0.9823 - val_loss: 0.2635 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "59176/59176 [==============================] - 10s 173us/step - loss: 0.0512 - acc: 0.9826 - val_loss: 0.2588 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "59176/59176 [==============================] - 10s 175us/step - loss: 0.0490 - acc: 0.9835 - val_loss: 0.2350 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "59176/59176 [==============================] - 10s 176us/step - loss: 0.0474 - acc: 0.9843 - val_loss: 0.2997 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "59176/59176 [==============================] - 10s 174us/step - loss: 0.0463 - acc: 0.9841 - val_loss: 0.2658 - val_acc: 0.9439\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "59176/59176 [==============================] - 10s 176us/step - loss: 0.0445 - acc: 0.9848 - val_loss: 0.2684 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "59176/59176 [==============================] - 10s 174us/step - loss: 0.0433 - acc: 0.9854 - val_loss: 0.3027 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "59176/59176 [==============================] - 10s 176us/step - loss: 0.0438 - acc: 0.9851 - val_loss: 0.3068 - val_acc: 0.9323\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbaebc0f60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.fit(x = X_train, \n",
    "                    y = to_categorical(Y_train), \n",
    "                    epochs = 15, \n",
    "                    batch_size = 128,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[checkpointer],\n",
    "                    validation_data=(X_test, to_categorical(Y_test)),\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.61      0.67      2475\n",
      "          1       0.95      0.98      0.96     18866\n",
      "\n",
      "avg / total       0.93      0.93      0.93     21341\n",
      "\n",
      "Weighted f1-score: 0.9288907890886872\n"
     ]
    }
   ],
   "source": [
    "Y_pred = detection_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.54      0.63      2475\n",
      "          1       0.94      0.98      0.96     18866\n",
      "\n",
      "avg / total       0.92      0.93      0.92     21341\n",
      "\n",
      "Weighted f1-score: 0.9209569053855828\n"
     ]
    }
   ],
   "source": [
    "detection_model_best = load_model('./data/model_ATSD_mult.hdf5')\n",
    "\n",
    "Y_pred = detection_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_d = Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data from subject 1\n",
      "\n",
      "Processing data from subject 2\n",
      "\n",
      "Processing data from subject 3\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadDataMultiple(label=label,\n",
    "                                                                                                        folder=folder,\n",
    "                                                                                                        window_size=window_size,\n",
    "                                                                                                        stride=stride,\n",
    "                                                                                                        make_binary=False,\n",
    "                                                                                                        null_class=False,\n",
    "                                                                                                        print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = models.ConvolutionalRecurrent((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "classification_model.compile(optimizer = Adam(lr=0.001),\n",
    "                             loss = \"categorical_crossentropy\", \n",
    "                             metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_ATSC_mult.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55852 samples, validate on 18866 samples\n",
      "Epoch 1/15\n",
      "55852/55852 [==============================] - 13s 237us/step - loss: 0.3996 - acc: 0.8448 - val_loss: 0.7824 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78243, saving model to ./data/model_ATSC_mult.hdf5\n",
      "Epoch 2/15\n",
      "55852/55852 [==============================] - 10s 174us/step - loss: 0.2724 - acc: 0.8966 - val_loss: 0.6166 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78243 to 0.61662, saving model to ./data/model_ATSC_mult.hdf5\n",
      "Epoch 3/15\n",
      "55852/55852 [==============================] - 10s 174us/step - loss: 0.2506 - acc: 0.9035 - val_loss: 0.7486 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "55852/55852 [==============================] - 10s 174us/step - loss: 0.2348 - acc: 0.9103 - val_loss: 0.5396 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.61662 to 0.53957, saving model to ./data/model_ATSC_mult.hdf5\n",
      "Epoch 5/15\n",
      "55852/55852 [==============================] - 10s 174us/step - loss: 0.2275 - acc: 0.9128 - val_loss: 0.6307 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "55852/55852 [==============================] - 10s 174us/step - loss: 0.2212 - acc: 0.9150 - val_loss: 0.7447 - val_acc: 0.8352\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "55852/55852 [==============================] - 10s 176us/step - loss: 0.2138 - acc: 0.9176 - val_loss: 0.7879 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "55852/55852 [==============================] - 10s 178us/step - loss: 0.2092 - acc: 0.9196 - val_loss: 0.7812 - val_acc: 0.8312\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "55852/55852 [==============================] - 10s 176us/step - loss: 0.2048 - acc: 0.9210 - val_loss: 0.8856 - val_acc: 0.8374\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "55852/55852 [==============================] - 10s 175us/step - loss: 0.1987 - acc: 0.9229 - val_loss: 0.8226 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "55852/55852 [==============================] - 10s 179us/step - loss: 0.1945 - acc: 0.9258 - val_loss: 0.7971 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "55852/55852 [==============================] - 10s 175us/step - loss: 0.1927 - acc: 0.9257 - val_loss: 0.8910 - val_acc: 0.8326\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "55852/55852 [==============================] - 10s 175us/step - loss: 0.1893 - acc: 0.9270 - val_loss: 0.8363 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "55852/55852 [==============================] - 10s 175us/step - loss: 0.1861 - acc: 0.9277 - val_loss: 0.7729 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "55852/55852 [==============================] - 10s 181us/step - loss: 0.1829 - acc: 0.9287 - val_loss: 0.8463 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbba8a9cc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.fit(x = X_train,\n",
    "                         y = to_categorical(Y_train), \n",
    "                         epochs = 15, \n",
    "                         batch_size = 128,\n",
    "                         verbose = 1,\n",
    "                         callbacks=[checkpointer],\n",
    "                         validation_data=(X_test, to_categorical(Y_test)),\n",
    "                         class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.95      0.82      7465\n",
      "          1       0.94      0.76      0.84      5445\n",
      "          2       0.93      0.72      0.81      4846\n",
      "          3       0.98      0.92      0.94      1110\n",
      "\n",
      "avg / total       0.86      0.83      0.83     18866\n",
      "\n",
      "Weighted f1-score: 0.832140837308565\n"
     ]
    }
   ],
   "source": [
    "Y_pred = classification_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.90      0.82      7465\n",
      "          1       0.87      0.84      0.85      5445\n",
      "          2       0.96      0.71      0.82      4846\n",
      "          3       0.95      0.93      0.94      1110\n",
      "\n",
      "avg / total       0.85      0.83      0.83     18866\n",
      "\n",
      "Weighted f1-score: 0.8338328873568304\n"
     ]
    }
   ],
   "source": [
    "classification_model_best = load_model('./data/model_ATSC_mult.hdf5')\n",
    "\n",
    "Y_pred = classification_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade of detection and classification\n",
    "The labels that have to be used for assessment are saved in Y_test_true. The labels predicted by the detection_model are saved instead in Y_pred_d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21341,) (21341,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_true.shape, Y_pred_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data from subject 1\n",
      "\n",
      "Processing data from subject 2\n",
      "\n",
      "Processing data from subject 3\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadDataMultiple(label=label,\n",
    "                                                                                                        folder=folder,\n",
    "                                                                                                        window_size=window_size,\n",
    "                                                                                                        stride=stride,\n",
    "                                                                                                        make_binary=True,\n",
    "                                                                                                        null_class=True,\n",
    "                                                                                                        print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Y_pred_d == 1)\n",
    "X_detected = X_test[mask, :, :]\n",
    "Y_pred_c = classification_model_best.predict_classes(X_detected)\n",
    "Y_pred_d[mask] = Y_pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.13      0.56      0.21      2475\n",
      "          1       0.12      0.09      0.10      7465\n",
      "          2       0.01      0.00      0.01      5445\n",
      "          3       0.02      0.01      0.01      4846\n",
      "          4       0.00      0.00      0.00      1110\n",
      "\n",
      "avg / total       0.06      0.10      0.06     21341\n",
      "\n",
      "Weighted f1-score: 0.06404905198508254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test_true, Y_pred_d))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test_true, Y_pred_d, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot classification instead had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data from subject 1\n",
      "\n",
      "Processing data from subject 2\n",
      "\n",
      "Processing data from subject 3\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadDataMultiple(label=label,\n",
    "                                                                                                        folder=folder,\n",
    "                                                                                                        window_size=window_size,\n",
    "                                                                                                        stride=stride,\n",
    "                                                                                                        make_binary=False,\n",
    "                                                                                                        null_class=True,\n",
    "                                                                                                        print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.60      0.66      2475\n",
      "          1       0.73      0.95      0.83      7465\n",
      "          2       0.82      0.69      0.75      5445\n",
      "          3       0.87      0.71      0.78      4846\n",
      "          4       0.93      0.89      0.91      1110\n",
      "\n",
      "avg / total       0.79      0.79      0.78     21341\n",
      "\n",
      "Weighted f1-score: 0.7813867327367077\n"
     ]
    }
   ],
   "source": [
    "oneshot_model_best = load_model('./data/model_AOS_mult.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"Weighted f1-score:\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
