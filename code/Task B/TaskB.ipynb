{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B - single subject - model ?\n",
    "\n",
    "## Notebook setup\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- label: index of feature column to be selected to perform activity detection, between [0,6]. The default value for task B is 6;\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "label = 6\n",
    "folder = \"../data/full/\"\n",
    "window_size = 15\n",
    "stride = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "import models\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we make use of some functions of Keras which have been removed, but of which the code is still available at https://github.com/keras-team/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7. These are used to evaulate the f1 score during training on batches of data: this is only an approximation though, which is the reason why they have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot classification\n",
    "Here classification is performed with null class.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session shapes:\n",
      "ADL1:   (45810, 110)\n",
      "ADL2:   (28996, 110)\n",
      "ADL3:   (30167, 110)\n",
      "ADL4:   (30228, 110)\n",
      "ADL5:   (27308, 110)\n",
      "Drill:  (52152, 110)\n",
      "\n",
      "Features: 110 \n",
      "Classes: 18 \n",
      "Fraction of labels:   [0.66779111 0.01365242 0.01473443 0.01120199 0.02020813 0.02074913\n",
      " 0.01183846 0.01613468 0.01307959 0.01377972 0.01584826 0.01225217\n",
      " 0.02132196 0.02199026 0.01152022 0.01724851 0.07889126 0.01775769]\n",
      "\n",
      "Features: 110 \n",
      "Classes: 18 \n",
      "Fraction of labels:   [0.77548892 0.00764885 0.00591047 0.00808344 0.01425467 0.01156019\n",
      " 0.00365059 0.01833985 0.00990874 0.0065189  0.00808344 0.00582355\n",
      " 0.01642764 0.01399392 0.00295524 0.01521078 0.06996958 0.00617123]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_train and Y_test contain the correct labels for each signals window. Y_test in particular will be used to evaluate predictions for both this (one-shot) and the two-steps models. For this reason it is here saved with a different name, to avoid having it being overwritten later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_true = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "oneshot_model.compile(optimizer = Adam(lr=0.001),\n",
    "                      loss = \"categorical_crossentropy\", \n",
    "                      metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./model_OS_1.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31423, 15, 110)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31423 samples, validate on 11505 samples\n",
      "Epoch 1/15\n",
      "31423/31423 [==============================] - 66s 2ms/step - loss: 0.5807 - acc: 0.8182 - val_loss: 0.3976 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39762, saving model to ./model_OS_1.hdf5\n",
      "Epoch 2/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.3671 - acc: 0.8703 - val_loss: 0.4739 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.2904 - acc: 0.8983 - val_loss: 0.4079 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.2497 - acc: 0.9134 - val_loss: 0.4069 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.2073 - acc: 0.9247 - val_loss: 0.4772 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1923 - acc: 0.9329 - val_loss: 0.4247 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1784 - acc: 0.9376 - val_loss: 0.4353 - val_acc: 0.8887\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1627 - acc: 0.9421 - val_loss: 0.4103 - val_acc: 0.8945\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1519 - acc: 0.9466 - val_loss: 0.4397 - val_acc: 0.8881\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1457 - acc: 0.9484 - val_loss: 0.5205 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1389 - acc: 0.9514 - val_loss: 0.5861 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1354 - acc: 0.9530 - val_loss: 0.4760 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1269 - acc: 0.9574 - val_loss: 0.4809 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1240 - acc: 0.9557 - val_loss: 0.6145 - val_acc: 0.8745\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1255 - acc: 0.9574 - val_loss: 0.4771 - val_acc: 0.8902\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29ca5a01f60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneshot_model.fit(x = X_train, \n",
    "                  y = to_categorical(Y_train),\n",
    "                  epochs = 15,\n",
    "                  batch_size = 16,\n",
    "                  verbose = 1,\n",
    "                  callbacks=[checkpointer],\n",
    "                  validation_data=(X_test, to_categorical(Y_test)),\n",
    "                  class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - passare class_weights a class report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95      8922\n",
      "          1       0.71      0.31      0.43        88\n",
      "          2       0.33      0.59      0.42        68\n",
      "          3       0.59      0.14      0.23        93\n",
      "          4       0.80      0.75      0.77       164\n",
      "          5       0.81      0.81      0.81       133\n",
      "          6       0.41      0.76      0.53        42\n",
      "          7       0.76      0.38      0.51       211\n",
      "          8       0.91      0.52      0.66       114\n",
      "          9       0.62      0.44      0.52        75\n",
      "         10       0.50      0.68      0.57        93\n",
      "         11       0.48      0.21      0.29        67\n",
      "         12       0.92      0.65      0.76       189\n",
      "         13       0.86      0.78      0.82       161\n",
      "         14       0.39      0.32      0.35        34\n",
      "         15       0.65      0.59      0.62       175\n",
      "         16       0.89      0.71      0.79       805\n",
      "         17       0.90      0.25      0.40        71\n",
      "\n",
      "avg / total       0.89      0.89      0.88     11505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = oneshot_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.96      0.93      8922\n",
      "          1       0.67      0.02      0.04        88\n",
      "          2       0.22      0.18      0.20        68\n",
      "          3       0.00      0.00      0.00        93\n",
      "          4       0.79      0.68      0.73       164\n",
      "          5       0.71      0.77      0.74       133\n",
      "          6       0.00      0.00      0.00        42\n",
      "          7       0.75      0.28      0.41       211\n",
      "          8       0.71      0.66      0.68       114\n",
      "          9       0.42      0.24      0.31        75\n",
      "         10       0.48      0.17      0.25        93\n",
      "         11       0.00      0.00      0.00        67\n",
      "         12       0.70      0.69      0.70       189\n",
      "         13       0.86      0.63      0.73       161\n",
      "         14       0.22      0.41      0.28        34\n",
      "         15       0.73      0.15      0.25       175\n",
      "         16       0.74      0.88      0.81       805\n",
      "         17       0.95      0.59      0.73        71\n",
      "\n",
      "avg / total       0.85      0.87      0.85     11505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "oneshot_model_best = load_model('./model_OS_1.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-steps classification\n",
    "## Activity detection\n",
    "This model performs a binary classification.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session shapes:\n",
      "ADL1:   (45810, 110)\n",
      "ADL2:   (28996, 110)\n",
      "ADL3:   (30167, 110)\n",
      "ADL4:   (30228, 110)\n",
      "ADL5:   (27308, 110)\n",
      "Drill:  (52152, 110)\n",
      "\n",
      "Features: 110 \n",
      "Classes: 2 \n",
      "Fraction of labels:   [0.66718646 0.33281354]\n",
      "\n",
      "Features: 110 \n",
      "Classes: 2 \n",
      "Fraction of labels:   [0.775402 0.224598]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=True,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "detection_model.compile(optimizer = Adam(lr=0.001),\n",
    "                        loss = \"categorical_crossentropy\", \n",
    "                        metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./model_TSD_1.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31423 samples, validate on 11505 samples\n",
      "Epoch 1/15\n",
      "31423/31423 [==============================] - 63s 2ms/step - loss: 0.2576 - acc: 0.8963 - val_loss: 0.2505 - val_acc: 0.8902\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25053, saving model to ./model_TSD_1.hdf5\n",
      "Epoch 2/15\n",
      "31423/31423 [==============================] - 60s 2ms/step - loss: 0.1817 - acc: 0.9268 - val_loss: 0.2182 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25053 to 0.21821, saving model to ./model_TSD_1.hdf5\n",
      "Epoch 3/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1547 - acc: 0.9391 - val_loss: 0.2944 - val_acc: 0.8965\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "31423/31423 [==============================] - 60s 2ms/step - loss: 0.1341 - acc: 0.9471 - val_loss: 0.2191 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1243 - acc: 0.9516 - val_loss: 0.2217 - val_acc: 0.9190\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1131 - acc: 0.9551 - val_loss: 0.2232 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.1036 - acc: 0.9584 - val_loss: 0.2563 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "31423/31423 [==============================] - 60s 2ms/step - loss: 0.1026 - acc: 0.9591 - val_loss: 0.2494 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "31423/31423 [==============================] - 60s 2ms/step - loss: 0.0943 - acc: 0.9623 - val_loss: 0.2344 - val_acc: 0.9213\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.0897 - acc: 0.9639 - val_loss: 0.2792 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.0888 - acc: 0.9648 - val_loss: 0.2350 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.0847 - acc: 0.9667 - val_loss: 0.2608 - val_acc: 0.9206\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.0810 - acc: 0.9684 - val_loss: 0.2311 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.0797 - acc: 0.9686 - val_loss: 0.2576 - val_acc: 0.9229\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "31423/31423 [==============================] - 61s 2ms/step - loss: 0.0777 - acc: 0.9688 - val_loss: 0.2864 - val_acc: 0.9216\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29ca59b0828>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.fit(x = X_train, \n",
    "                    y = to_categorical(Y_train), \n",
    "                    epochs = 15, \n",
    "                    batch_size = 16,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[checkpointer],\n",
    "                    validation_data=(X_test, to_categorical(Y_test)),\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.97      0.95      8921\n",
      "          1       0.88      0.75      0.81      2584\n",
      "\n",
      "avg / total       0.92      0.92      0.92     11505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = detection_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94      8921\n",
      "          1       0.80      0.76      0.78      2584\n",
      "\n",
      "avg / total       0.90      0.90      0.90     11505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detection_model_best = load_model('./model_TSD_1.hdf5')\n",
    "\n",
    "Y_pred = detection_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_d = Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session shapes:\n",
      "ADL1:   (45810, 110)\n",
      "ADL2:   (28996, 110)\n",
      "ADL3:   (30167, 110)\n",
      "ADL4:   (30228, 110)\n",
      "ADL5:   (27308, 110)\n",
      "Drill:  (52152, 110)\n",
      "\n",
      "Features: 110 \n",
      "Classes: 17 \n",
      "Fraction of labels:   [0.04109589 0.04435291 0.0337197  0.06082958 0.06245809 0.0356356\n",
      " 0.04856787 0.03937159 0.04147907 0.04770572 0.03688093 0.06418239\n",
      " 0.06619408 0.03467765 0.05192068 0.23747485 0.0534534 ]\n",
      "\n",
      "Features: 110 \n",
      "Classes: 17 \n",
      "Fraction of labels:   [0.03406891 0.02632598 0.03600465 0.06349206 0.05149051 0.01626016\n",
      " 0.08168796 0.04413473 0.029036   0.03600465 0.02593883 0.07317073\n",
      " 0.06233062 0.01316299 0.06775068 0.31165312 0.02748742]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=False,\n",
    "                                                                                                print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "classification_model.compile(optimizer = Adam(lr=0.001),\n",
    "                             loss = \"categorical_crossentropy\", \n",
    "                             metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./model_TSC_1.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10439 samples, validate on 2583 samples\n",
      "Epoch 1/15\n",
      "10439/10439 [==============================] - 24s 2ms/step - loss: 0.9066 - acc: 0.6635 - val_loss: 0.9047 - val_acc: 0.7120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90465, saving model to ./model_TSC_1.hdf5\n",
      "Epoch 2/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.5395 - acc: 0.7932 - val_loss: 0.8344 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.90465 to 0.83444, saving model to ./model_TSC_1.hdf5\n",
      "Epoch 3/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.4156 - acc: 0.8491 - val_loss: 0.7916 - val_acc: 0.7542\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.83444 to 0.79159, saving model to ./model_TSC_1.hdf5\n",
      "Epoch 4/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.3152 - acc: 0.8863 - val_loss: 0.8323 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.2618 - acc: 0.9100 - val_loss: 0.7750 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.79159 to 0.77505, saving model to ./model_TSC_1.hdf5\n",
      "Epoch 6/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.2298 - acc: 0.9181 - val_loss: 0.8972 - val_acc: 0.7685\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.2043 - acc: 0.9289 - val_loss: 0.9734 - val_acc: 0.7731\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1750 - acc: 0.9413 - val_loss: 0.8754 - val_acc: 0.7851\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1579 - acc: 0.9467 - val_loss: 1.0664 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1454 - acc: 0.9533 - val_loss: 0.9179 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1454 - acc: 0.9517 - val_loss: 1.0469 - val_acc: 0.7588\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1330 - acc: 0.9571 - val_loss: 0.9530 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1177 - acc: 0.9621 - val_loss: 1.0562 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1173 - acc: 0.9618 - val_loss: 1.1471 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "10439/10439 [==============================] - 20s 2ms/step - loss: 0.1272 - acc: 0.9600 - val_loss: 1.0446 - val_acc: 0.7731\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29cd25c4a20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.fit(x = X_train,\n",
    "                         y = to_categorical(Y_train), \n",
    "                         epochs = 15, \n",
    "                         batch_size = 16,\n",
    "                         verbose = 1,\n",
    "                         callbacks=[checkpointer],\n",
    "                         validation_data=(X_test, to_categorical(Y_test)),\n",
    "                         class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.18      0.23        88\n",
      "          1       0.61      0.53      0.57        68\n",
      "          2       0.12      0.01      0.02        93\n",
      "          3       0.75      0.78      0.76       164\n",
      "          4       0.70      0.90      0.79       133\n",
      "          5       0.57      0.90      0.70        42\n",
      "          6       0.72      0.73      0.72       211\n",
      "          7       0.81      0.89      0.85       114\n",
      "          8       0.50      0.52      0.51        75\n",
      "          9       0.55      0.84      0.66        93\n",
      "         10       0.36      0.40      0.38        67\n",
      "         11       0.82      0.59      0.69       189\n",
      "         12       0.86      0.81      0.83       161\n",
      "         13       0.17      0.12      0.14        34\n",
      "         14       0.72      0.81      0.76       175\n",
      "         15       0.96      1.00      0.98       805\n",
      "         16       0.97      0.93      0.95        71\n",
      "\n",
      "avg / total       0.75      0.77      0.76      2583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = classification_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.44      0.49        88\n",
      "          1       0.67      0.63      0.65        68\n",
      "          2       0.55      0.34      0.42        93\n",
      "          3       0.76      0.80      0.78       164\n",
      "          4       0.82      0.73      0.77       133\n",
      "          5       0.80      0.88      0.84        42\n",
      "          6       0.82      0.62      0.70       211\n",
      "          7       0.88      0.87      0.87       114\n",
      "          8       0.49      0.51      0.50        75\n",
      "          9       0.54      0.86      0.66        93\n",
      "         10       0.23      0.46      0.31        67\n",
      "         11       0.77      0.64      0.70       189\n",
      "         12       0.78      0.89      0.83       161\n",
      "         13       0.75      0.09      0.16        34\n",
      "         14       0.76      0.82      0.79       175\n",
      "         15       0.96      1.00      0.98       805\n",
      "         16       0.98      0.73      0.84        71\n",
      "\n",
      "avg / total       0.80      0.78      0.78      2583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_model_best = load_model('./model_TSC_1.hdf5')\n",
    "\n",
    "Y_pred = classification_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade of detection and classification\n",
    "The labels that have to be used for assessment are saved in Y_test_true. The labels predicted by the detection_model are saved instead in Y_pred_d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11505,) (11505,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_true.shape, Y_pred_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=True,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Y_pred_d == 1)\n",
    "X_detected = X_test[mask, :, :]\n",
    "Y_pred_c = classification_model_best.predict_classes(X_detected)\n",
    "Y_pred_d[mask] = Y_pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94      8922\n",
      "          1       0.04      0.02      0.03        88\n",
      "          2       0.00      0.00      0.00        68\n",
      "          3       0.00      0.00      0.00        93\n",
      "          4       0.02      0.01      0.01       164\n",
      "          5       0.00      0.00      0.00       133\n",
      "          6       0.00      0.00      0.00        42\n",
      "          7       0.01      0.00      0.01       211\n",
      "          8       0.00      0.00      0.00       114\n",
      "          9       0.01      0.01      0.01        75\n",
      "         10       0.03      0.04      0.04        93\n",
      "         11       0.00      0.00      0.00        67\n",
      "         12       0.01      0.01      0.01       189\n",
      "         13       0.00      0.00      0.00       161\n",
      "         14       0.00      0.00      0.00        34\n",
      "         15       0.00      0.00      0.00       175\n",
      "         16       0.00      0.00      0.00       805\n",
      "         17       0.00      0.00      0.00        71\n",
      "\n",
      "avg / total       0.72      0.74      0.73     11505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test_true, Y_pred_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot classification instead had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.96      0.93      8922\n",
      "          1       0.67      0.02      0.04        88\n",
      "          2       0.22      0.18      0.20        68\n",
      "          3       0.00      0.00      0.00        93\n",
      "          4       0.79      0.68      0.73       164\n",
      "          5       0.71      0.77      0.74       133\n",
      "          6       0.00      0.00      0.00        42\n",
      "          7       0.75      0.28      0.41       211\n",
      "          8       0.71      0.66      0.68       114\n",
      "          9       0.42      0.24      0.31        75\n",
      "         10       0.48      0.17      0.25        93\n",
      "         11       0.00      0.00      0.00        67\n",
      "         12       0.70      0.69      0.70       189\n",
      "         13       0.86      0.63      0.73       161\n",
      "         14       0.22      0.41      0.28        34\n",
      "         15       0.73      0.15      0.25       175\n",
      "         16       0.74      0.88      0.81       805\n",
      "         17       0.95      0.59      0.73        71\n",
      "\n",
      "avg / total       0.85      0.87      0.85     11505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "oneshot_model_best = load_model('./model_OS_1.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
