{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B - single subject - model ?\n",
    "\n",
    "## Notebook setup\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- label: index of feature column to be selected to perform activity detection, between [0,6]. The default value for task B is 6;\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 2\n",
    "label = 6\n",
    "folder = \"../data/full/\"\n",
    "window_size = 15\n",
    "stride = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "import models\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we make use of some functions of Keras which have been removed, but of which the code is still available at https://github.com/keras-team/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7. These are used to evaulate the f1 score during training on batches of data: this is only an approximation though, which is the reason why they have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not(os.path.exists(\"./data\")):\n",
    "    os.mkdir(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot classification\n",
    "Here classification is performed with null class.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_train and Y_test contain the correct labels for each signals window. Y_test in particular will be used to evaluate predictions for both this (one-shot) and the two-steps models. For this reason it is here saved with a different name, to avoid having it being overwritten later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_true = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "oneshot_model.compile(optimizer = Adam(lr=0.001),\n",
    "                      loss = \"categorical_crossentropy\", \n",
    "                      metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_BOS_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29159, 15, 110)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29159 samples, validate on 11542 samples\n",
      "Epoch 1/15\n",
      "29159/29159 [==============================] - 62s 2ms/step - loss: 1.2292 - acc: 0.6530 - val_loss: 0.6553 - val_acc: 0.8259\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65528, saving model to ./data/model_BOS_2.hdf5\n",
      "Epoch 2/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.9829 - acc: 0.6915 - val_loss: 0.7193 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.8749 - acc: 0.7167 - val_loss: 0.6991 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.7943 - acc: 0.7372 - val_loss: 0.7343 - val_acc: 0.8029\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.7386 - acc: 0.7531 - val_loss: 0.7230 - val_acc: 0.8055\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.6953 - acc: 0.7654 - val_loss: 0.7522 - val_acc: 0.8005\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "29159/29159 [==============================] - 56s 2ms/step - loss: 0.6478 - acc: 0.7779 - val_loss: 0.7537 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "29159/29159 [==============================] - 56s 2ms/step - loss: 0.6191 - acc: 0.7870 - val_loss: 0.7525 - val_acc: 0.8124\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.5936 - acc: 0.7932 - val_loss: 0.8349 - val_acc: 0.8096\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.5689 - acc: 0.8024 - val_loss: 0.8029 - val_acc: 0.8123\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.5415 - acc: 0.8106 - val_loss: 0.8337 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.5234 - acc: 0.8148 - val_loss: 0.8249 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.5061 - acc: 0.8207 - val_loss: 0.8929 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.4903 - acc: 0.8259 - val_loss: 0.8123 - val_acc: 0.8097\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.4650 - acc: 0.8374 - val_loss: 0.9137 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1af43ddaf60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneshot_model.fit(x = X_train, \n",
    "                  y = to_categorical(Y_train),\n",
    "                  epochs = 15,\n",
    "                  batch_size = 16,\n",
    "                  verbose = 1,\n",
    "                  callbacks=[checkpointer],\n",
    "                  validation_data=(X_test, to_categorical(Y_test)),\n",
    "                  class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - passare class_weights a class report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.91      0.91      9699\n",
      "          1       0.20      0.14      0.16       108\n",
      "          2       0.03      0.01      0.02        76\n",
      "          3       0.00      0.00      0.00        34\n",
      "          4       0.06      0.08      0.07        51\n",
      "          5       0.25      0.34      0.29       109\n",
      "          6       0.01      0.03      0.02        35\n",
      "          7       0.44      0.44      0.44       212\n",
      "          8       0.11      0.12      0.11       119\n",
      "          9       0.17      0.16      0.16       141\n",
      "         10       0.00      0.00      0.00        74\n",
      "         11       0.00      0.00      0.00        47\n",
      "         12       0.07      0.24      0.11        62\n",
      "         13       0.18      0.38      0.24       110\n",
      "         14       0.07      0.10      0.08        51\n",
      "         15       0.37      0.29      0.32       262\n",
      "         16       0.10      0.04      0.05       185\n",
      "         17       0.07      0.07      0.07       167\n",
      "\n",
      "avg / total       0.80      0.79      0.80     11542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = oneshot_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91      9699\n",
      "          1       0.00      0.00      0.00       108\n",
      "          2       0.00      0.00      0.00        76\n",
      "          3       0.00      0.00      0.00        34\n",
      "          4       0.00      0.00      0.00        51\n",
      "          5       0.30      0.03      0.05       109\n",
      "          6       0.00      0.00      0.00        35\n",
      "          7       0.45      0.33      0.38       212\n",
      "          8       0.00      0.00      0.00       119\n",
      "          9       0.18      0.06      0.09       141\n",
      "         10       0.00      0.00      0.00        74\n",
      "         11       0.50      0.02      0.04        47\n",
      "         12       0.00      0.00      0.00        62\n",
      "         13       0.16      0.11      0.13       110\n",
      "         14       0.11      0.08      0.09        51\n",
      "         15       0.19      0.14      0.16       262\n",
      "         16       0.00      0.00      0.00       185\n",
      "         17       0.00      0.00      0.00       167\n",
      "\n",
      "avg / total       0.75      0.83      0.78     11542\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "oneshot_model_best = load_model('./data/model_BOS_2.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-steps classification\n",
    "## Activity detection\n",
    "This model performs a binary classification.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=True,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "detection_model.compile(optimizer = Adam(lr=0.001),\n",
    "                        loss = \"categorical_crossentropy\", \n",
    "                        metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_BTSD_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29159 samples, validate on 11542 samples\n",
      "Epoch 1/15\n",
      "29159/29159 [==============================] - 60s 2ms/step - loss: 0.5305 - acc: 0.7361 - val_loss: 0.3238 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32378, saving model to ./data/model_BTSD_2.hdf5\n",
      "Epoch 2/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.4742 - acc: 0.7718 - val_loss: 0.3091 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32378 to 0.30906, saving model to ./data/model_BTSD_2.hdf5\n",
      "Epoch 3/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.4384 - acc: 0.7944 - val_loss: 0.3626 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.4132 - acc: 0.8135 - val_loss: 0.3595 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.3978 - acc: 0.8172 - val_loss: 0.3631 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.3731 - acc: 0.8305 - val_loss: 0.3350 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "29159/29159 [==============================] - 57s 2ms/step - loss: 0.3589 - acc: 0.8398 - val_loss: 0.3623 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "29136/29159 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8487"
     ]
    }
   ],
   "source": [
    "detection_model.fit(x = X_train, \n",
    "                    y = to_categorical(Y_train), \n",
    "                    epochs = 15, \n",
    "                    batch_size = 16,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[checkpointer],\n",
    "                    validation_data=(X_test, to_categorical(Y_test)),\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = detection_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model_best = load_model('./data/model_BTSD_2.hdf5')\n",
    "\n",
    "Y_pred = detection_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_d = Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=False,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "classification_model.compile(optimizer = Adam(lr=0.001),\n",
    "                             loss = \"categorical_crossentropy\", \n",
    "                             metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_BTSC_2.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.fit(x = X_train,\n",
    "                         y = to_categorical(Y_train), \n",
    "                         epochs = 15, \n",
    "                         batch_size = 16,\n",
    "                         verbose = 1,\n",
    "                         callbacks=[checkpointer],\n",
    "                         validation_data=(X_test, to_categorical(Y_test)),\n",
    "                         class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = classification_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_best = load_model('./data/model_BTSC_2.hdf5')\n",
    "\n",
    "Y_pred = classification_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade of detection and classification\n",
    "The labels that have to be used for assessment are saved in Y_test_true. The labels predicted by the detection_model are saved instead in Y_pred_d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test_true.shape, Y_pred_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=True,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Y_pred_d == 1)\n",
    "X_detected = X_test[mask, :, :]\n",
    "Y_pred_c = classification_model_best.predict_classes(X_detected)\n",
    "Y_pred_d[mask] = Y_pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test_true, Y_pred_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot classification instead had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot_model_best = load_model('./data/model_BOS_2.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
