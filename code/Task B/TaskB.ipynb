{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B - single subject - model ?\n",
    "\n",
    "## Notebook setup\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- label: index of feature column to be selected to perform activity detection, between [0,6]. The default value for task B is 6;\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 3\n",
    "label = 6\n",
    "folder = \"../data/full/\"\n",
    "window_size = 15\n",
    "stride = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "import models\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we make use of some functions of Keras which have been removed, but of which the code is still available at https://github.com/keras-team/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7. These are used to evaulate the f1 score during training on batches of data: this is only an approximation though, which is the reason why they have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not(os.path.exists(\"./data\")):\n",
    "    os.mkdir(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot classification\n",
    "Here classification is performed with null class.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_train and Y_test contain the correct labels for each signals window. Y_test in particular will be used to evaluate predictions for both this (one-shot) and the two-steps models. For this reason it is here saved with a different name, to avoid having it being overwritten later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_true = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "oneshot_model.compile(optimizer = Adam(lr=0.001),\n",
    "                      loss = \"categorical_crossentropy\", \n",
    "                      metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_BOS_3.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30017, 15, 110)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30017 samples, validate on 9799 samples\n",
      "Epoch 1/15\n",
      "30017/30017 [==============================] - 63s 2ms/step - loss: 0.6991 - acc: 0.7850 - val_loss: 0.5491 - val_acc: 0.8186\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54907, saving model to ./data/model_BOS_3.hdf5\n",
      "Epoch 2/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.4411 - acc: 0.8524 - val_loss: 0.5135 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54907 to 0.51354, saving model to ./data/model_BOS_3.hdf5\n",
      "Epoch 3/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.3425 - acc: 0.8833 - val_loss: 0.5229 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.2877 - acc: 0.9033 - val_loss: 0.5174 - val_acc: 0.8394\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.2614 - acc: 0.9100 - val_loss: 0.4814 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51354 to 0.48138, saving model to ./data/model_BOS_3.hdf5\n",
      "Epoch 6/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.2330 - acc: 0.9192 - val_loss: 0.5036 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.2141 - acc: 0.9265 - val_loss: 0.5449 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.2023 - acc: 0.9292 - val_loss: 0.5337 - val_acc: 0.8588\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.1955 - acc: 0.9330 - val_loss: 0.5829 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1857 - acc: 0.9346 - val_loss: 0.5267 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1814 - acc: 0.9370 - val_loss: 0.5524 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1715 - acc: 0.9395 - val_loss: 0.5190 - val_acc: 0.8755\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.1672 - acc: 0.9412 - val_loss: 0.6347 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1630 - acc: 0.9436 - val_loss: 0.5352 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1606 - acc: 0.9436 - val_loss: 0.5784 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c93dfabf60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneshot_model.fit(x = X_train, \n",
    "                  y = to_categorical(Y_train),\n",
    "                  epochs = 15,\n",
    "                  batch_size = 16,\n",
    "                  verbose = 1,\n",
    "                  callbacks=[checkpointer],\n",
    "                  validation_data=(X_test, to_categorical(Y_test)),\n",
    "                  class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - passare class_weights a class report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.94      0.93      7668\n",
      "          1       0.49      0.47      0.48        80\n",
      "          2       0.67      0.72      0.69        69\n",
      "          3       0.75      0.56      0.64        32\n",
      "          4       0.47      0.49      0.48        91\n",
      "          5       0.80      0.82      0.81        89\n",
      "          6       0.37      0.35      0.36        69\n",
      "          7       0.52      0.79      0.62       171\n",
      "          8       0.86      0.18      0.29       136\n",
      "          9       0.61      0.45      0.52        97\n",
      "         10       0.48      0.42      0.45        84\n",
      "         11       0.36      0.78      0.49        50\n",
      "         12       0.48      0.82      0.61        72\n",
      "         13       0.76      0.88      0.82       117\n",
      "         14       0.49      0.45      0.47        42\n",
      "         15       0.62      0.45      0.52       285\n",
      "         16       0.81      0.58      0.67       575\n",
      "         17       0.62      0.69      0.66        72\n",
      "\n",
      "avg / total       0.86      0.86      0.85      9799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = oneshot_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93      7668\n",
      "          1       0.44      0.54      0.48        80\n",
      "          2       0.57      0.64      0.60        69\n",
      "          3       0.44      0.56      0.49        32\n",
      "          4       0.49      0.73      0.58        91\n",
      "          5       0.59      0.96      0.73        89\n",
      "          6       0.52      0.48      0.50        69\n",
      "          7       0.57      0.74      0.64       171\n",
      "          8       0.83      0.51      0.64       136\n",
      "          9       0.37      0.47      0.42        97\n",
      "         10       0.68      0.38      0.49        84\n",
      "         11       0.50      0.14      0.22        50\n",
      "         12       0.52      0.79      0.63        72\n",
      "         13       0.64      0.68      0.66       117\n",
      "         14       0.41      0.74      0.53        42\n",
      "         15       0.70      0.39      0.50       285\n",
      "         16       0.76      0.74      0.75       575\n",
      "         17       0.83      0.76      0.80        72\n",
      "\n",
      "avg / total       0.87      0.87      0.86      9799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oneshot_model_best = load_model('./data/model_BOS_3.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-steps classification\n",
    "## Activity detection\n",
    "This model performs a binary classification.\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=True,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "detection_model.compile(optimizer = Adam(lr=0.001),\n",
    "                        loss = \"categorical_crossentropy\", \n",
    "                        metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_BTSD_3.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30017 samples, validate on 9799 samples\n",
      "Epoch 1/15\n",
      "30017/30017 [==============================] - 61s 2ms/step - loss: 0.3339 - acc: 0.8598 - val_loss: 0.3233 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32334, saving model to ./data/model_BTSD_3.hdf5\n",
      "Epoch 2/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.2435 - acc: 0.9019 - val_loss: 0.2952 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32334 to 0.29517, saving model to ./data/model_BTSD_3.hdf5\n",
      "Epoch 3/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.2121 - acc: 0.9116 - val_loss: 0.3197 - val_acc: 0.8702\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.1908 - acc: 0.9218 - val_loss: 0.3108 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1721 - acc: 0.9317 - val_loss: 0.3130 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1627 - acc: 0.9325 - val_loss: 0.2820 - val_acc: 0.8830\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29517 to 0.28203, saving model to ./data/model_BTSD_3.hdf5\n",
      "Epoch 7/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1494 - acc: 0.9402 - val_loss: 0.2826 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.1446 - acc: 0.9421 - val_loss: 0.3049 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.1377 - acc: 0.9446 - val_loss: 0.3078 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1313 - acc: 0.9473 - val_loss: 0.3703 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1250 - acc: 0.9486 - val_loss: 0.3079 - val_acc: 0.8854\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "30017/30017 [==============================] - 57s 2ms/step - loss: 0.1224 - acc: 0.9500 - val_loss: 0.2647 - val_acc: 0.8968\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.28203 to 0.26470, saving model to ./data/model_BTSD_3.hdf5\n",
      "Epoch 13/15\n",
      "30017/30017 [==============================] - 58s 2ms/step - loss: 0.1215 - acc: 0.9517 - val_loss: 0.3141 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/15\n",
      "17776/30017 [================>.............] - ETA: 22s - loss: 0.1117 - acc: 0.9553"
     ]
    }
   ],
   "source": [
    "detection_model.fit(x = X_train, \n",
    "                    y = to_categorical(Y_train), \n",
    "                    epochs = 15, \n",
    "                    batch_size = 16,\n",
    "                    verbose = 1,\n",
    "                    callbacks=[checkpointer],\n",
    "                    validation_data=(X_test, to_categorical(Y_test)),\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = detection_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model_best = load_model('./data/model_BTSD_3.hdf5')\n",
    "\n",
    "Y_pred = detection_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_d = Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=False,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = models.MotionDetection((window_size, n_features), n_classes, print_info=False)\n",
    "\n",
    "classification_model.compile(optimizer = Adam(lr=0.001),\n",
    "                             loss = \"categorical_crossentropy\", \n",
    "                             metrics = [\"accuracy\"])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/model_BTSC_3.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.fit(x = X_train,\n",
    "                         y = to_categorical(Y_train), \n",
    "                         epochs = 15, \n",
    "                         batch_size = 16,\n",
    "                         verbose = 1,\n",
    "                         callbacks=[checkpointer],\n",
    "                         validation_data=(X_test, to_categorical(Y_test)),\n",
    "                         class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = classification_model.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_best = load_model('./data/model_BTSC_3.hdf5')\n",
    "\n",
    "Y_pred = classification_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade of detection and classification\n",
    "The labels that have to be used for assessment are saved in Y_test_true. The labels predicted by the detection_model are saved instead in Y_pred_d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test_true.shape, Y_pred_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=True,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Y_pred_d == 1)\n",
    "X_detected = X_test[mask, :, :]\n",
    "Y_pred_c = classification_model_best.predict_classes(X_detected)\n",
    "Y_pred_d[mask] = Y_pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test_true, Y_pred_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot classification instead had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneshot_model_best = load_model('./data/model_BOS_3.hdf5')\n",
    "\n",
    "Y_pred = oneshot_model_best.predict_classes(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
