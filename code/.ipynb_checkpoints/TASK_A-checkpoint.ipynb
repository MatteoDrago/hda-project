{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3: TASK A\n",
    "## Classification of Modes of Locomotion\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: column of features to be selected to perform activity detection, between [0,6];\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 2\n",
    "folder = \"./data/\"\n",
    "trim_zeros = True\n",
    "label_col = 0     # default for task A\n",
    "window_size = 15\n",
    "stride = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0871f31016f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# import all sessions for a subject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata6\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# create training set and test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - Università degli Studi di Padova\\Università\\Magistrale\\Human Data Analytics\\hda-project\\code\\utils.py\u001b[0m in \u001b[0;36mloadData\u001b[1;34m(subject, folder)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mfilename_6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"S\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-Drill.mat\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mdata2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdata3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'labels'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \"\"\"\n\u001b[0;32m    140\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mMR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[1;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mbyte_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "make_binary = False\n",
    "\n",
    "# import all sessions for a subject\n",
    "(data1, data2, data3, data4, data5, data6) = utils.loadData(subject, folder=folder)\n",
    "\n",
    "# create training set and test set\n",
    "X_train = np.concatenate((data1['features_interp'],\\\n",
    "                          data2['features_interp'],\\\n",
    "                          data3['features_interp'],\\\n",
    "                          data6['features_interp']), axis=0)\n",
    "\n",
    "Y_train = np.concatenate((data1['labels_cut'][:,label_col],\\\n",
    "                          data2['labels_cut'][:,label_col],\\\n",
    "                          data3['labels_cut'][:,label_col],\\\n",
    "                          data6['labels_cut'][:,label_col]), axis=0)\n",
    "\n",
    "X_test = np.concatenate((data4['features_interp'],\\\n",
    "                         data5['features_interp']), axis=0)\n",
    "\n",
    "Y_test = np.concatenate((data4['labels_cut'][:,label_col],\\\n",
    "                         data5['labels_cut'][:,label_col]))\n",
    "\n",
    "features = X_test.shape[1]\n",
    "print(\"\\nTraining samples: \", X_train.shape[0],\\\n",
    "      \"\\nTest samples:      \", X_test.shape[0],\\\n",
    "      \"\\nFeatures:            \", features)\n",
    "\n",
    "# decision to overcome the problem of entire missing columns\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "# features normalization\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train =scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# make the problem binary\n",
    "if make_binary:\n",
    "    Y_train[Y_train != 0] = 1\n",
    "    Y_test[Y_test != 0] = 1\n",
    "\n",
    "# switch to one hot encoded labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "Y_train_oh = onehot_encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "Y_test_oh = onehot_encoder.fit_transform(Y_test.reshape(-1, 1))\n",
    "print(\"\\nClasses in training set: \", Y_train_oh.shape[1],\\\n",
    "      \"\\nClasses in test set:     \", Y_test_oh.shape[1])\n",
    "\n",
    "print(\"Training set:\")\n",
    "X_train_s, Y_train_s = utils.prepareData(X_train, Y_train_oh, window_size, stride, shuffle=False, null_class=False)\n",
    "print(\"\\nTest set:\")\n",
    "X_test_s, Y_test_s = utils.prepareData(X_test, Y_test_oh, window_size, stride, shuffle=False, null_class=False)\n",
    "# add bars plot\n",
    "\n",
    "classes = np.max((Y_train_s.shape[1], Y_test_s.shape[1]))\n",
    "\n",
    "model_unidim = utils.Model1D((window_size, features), classes)\n",
    "\n",
    "opt = Adam(lr=0.01)\n",
    "model_unidim.compile(optimizer = opt, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model_unidim.fit(x = X_train_s, y = Y_train_s, epochs = 50, batch_size = 128, validation_data=(X_test_s, Y_test_s))\n",
    "\n",
    "# predict labels\n",
    "Y_pred_s = model_unidim.predict(X_test_s)\n",
    "\n",
    "# print results\n",
    "#reverse the one-ot encoder procedure\n",
    "Y_test_hard = np.argmax(Y_test_s, axis=1)\n",
    "Y_pred_hard = np.argmax(Y_pred_s, axis=1)\n",
    "\n",
    "print(\"F1-measure: \", utils.f1_score(Y_test_hard, Y_pred_hard, average='weighted'))\n",
    "print(\"AUC w.r. to each class: \", utils.AUC(Y_test_s, Y_pred_s, classes))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cnf_matrix = utils.confusion_matrix(Y_test_hard, Y_pred_hard)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=[1,2],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
