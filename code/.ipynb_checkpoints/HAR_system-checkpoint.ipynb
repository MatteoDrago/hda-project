{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR system\n",
    "\n",
    "#### HDA project - Riccardo Lincetto, Matteo Drago\n",
    "\n",
    "This notebook uses the OPPORTUNITY Activity Recognition Dataset to perform a classification over locomotion modes and gestures from on-body sensors.\n",
    "This benchmark dataset was introduced in 2011 with a challenge: all the information concerning that can be found at http://opportunity-project.eu/challenge.\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = [1]\n",
    "folder = \"./data/full/\"\n",
    "label = 0     # 0 for task A, 6 for task B\n",
    "window_size = 15\n",
    "stride = 5\n",
    "make_binary = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import deeplearning\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.activations import relu\n",
    "from keras.layers import Conv1D, Conv2D, BatchNormalization, Dropout, LeakyReLU, Flatten, Activation, Dense, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers import Conv2D, BatchNormalization, Dropout, LeakyReLU, Flatten, Activation, Dense, MaxPooling2D, LSTM, Reshape\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (None, 15, 110, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 108, 50)        1700      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 108, 50)        0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 2, 5400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 2, 20)             433680    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               10752     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 450,442\n",
      "Trainable params: 450,440\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features = 110 #number of features taken into consideration for the solution of the problem\n",
    "n_classes = 2\n",
    "\n",
    "detection_model = deeplearning.MotionDetection((window_size,n_features,1), n_classes)\n",
    "detection_model.summary() # model visualization\n",
    "\n",
    "detection_model.compile(optimizer = Adam(lr=0.01), \n",
    "                   loss = \"categorical_crossentropy\", \n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User  1\n",
      "Training samples:  157125 \n",
      "Test samples:       57536 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (31422, 15, 110) \n",
      "Dataset of Labels have shape:    (31422, 2) \n",
      "Fraction of labels:   [0.11036853 0.88963147]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (11504, 15, 110) \n",
      "Dataset of Labels have shape:    (11504, 2) \n",
      "Fraction of labels:   [0.1772427 0.8227573]\n",
      "Train on 31422 samples, validate on 11504 samples\n",
      "Epoch 1/20\n",
      "31422/31422 [==============================] - 9s 271us/step - loss: 0.2014 - acc: 0.9259 - val_loss: 0.1962 - val_acc: 0.9126\n",
      "Epoch 2/20\n",
      "31422/31422 [==============================] - 5s 148us/step - loss: 0.1372 - acc: 0.9550 - val_loss: 0.1979 - val_acc: 0.9259\n",
      "Epoch 3/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1252 - acc: 0.9582 - val_loss: 0.2076 - val_acc: 0.9322\n",
      "Epoch 4/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1220 - acc: 0.9600 - val_loss: 0.2081 - val_acc: 0.9113\n",
      "Epoch 5/20\n",
      "31422/31422 [==============================] - 5s 148us/step - loss: 0.1142 - acc: 0.9608 - val_loss: 0.1963 - val_acc: 0.9303\n",
      "Epoch 6/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1142 - acc: 0.9601 - val_loss: 0.1756 - val_acc: 0.9418\n",
      "Epoch 7/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1096 - acc: 0.9624 - val_loss: 0.2103 - val_acc: 0.9199\n",
      "Epoch 8/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1125 - acc: 0.9601 - val_loss: 0.1670 - val_acc: 0.9359\n",
      "Epoch 9/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1188 - acc: 0.9589 - val_loss: 0.2646 - val_acc: 0.9072\n",
      "Epoch 10/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1098 - acc: 0.9643 - val_loss: 0.1858 - val_acc: 0.9362\n",
      "Epoch 11/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1116 - acc: 0.9635 - val_loss: 0.1727 - val_acc: 0.9283\n",
      "Epoch 12/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1197 - acc: 0.9589 - val_loss: 0.2153 - val_acc: 0.9338\n",
      "Epoch 13/20\n",
      "31422/31422 [==============================] - 5s 148us/step - loss: 0.1037 - acc: 0.9644 - val_loss: 0.2031 - val_acc: 0.8971\n",
      "Epoch 14/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1109 - acc: 0.9608 - val_loss: 0.1756 - val_acc: 0.9355\n",
      "Epoch 15/20\n",
      "31422/31422 [==============================] - 5s 148us/step - loss: 0.1107 - acc: 0.9638 - val_loss: 0.1687 - val_acc: 0.9429\n",
      "Epoch 16/20\n",
      "31422/31422 [==============================] - 5s 150us/step - loss: 0.1184 - acc: 0.9651 - val_loss: 0.1731 - val_acc: 0.9364\n",
      "Epoch 17/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1179 - acc: 0.9626 - val_loss: 0.1716 - val_acc: 0.9406\n",
      "Epoch 18/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1175 - acc: 0.9639 - val_loss: 0.1950 - val_acc: 0.9202\n",
      "Epoch 19/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1113 - acc: 0.9613 - val_loss: 0.1617 - val_acc: 0.9374\n",
      "Epoch 20/20\n",
      "31422/31422 [==============================] - 5s 147us/step - loss: 0.1091 - acc: 0.9647 - val_loss: 0.1824 - val_acc: 0.9268\n"
     ]
    }
   ],
   "source": [
    "for s in subject:\n",
    "    \n",
    "    print(\"\\nUser \", s)\n",
    "    \n",
    "    [x_train, y_train, x_test, y_test, n_classes] = utils.preprocessing(s,\n",
    "                                                                        folder,\n",
    "                                                                        label,\n",
    "                                                                        window_size,\n",
    "                                                                        stride,\n",
    "                                                                        make_binary = True)\n",
    "\n",
    "    input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "    input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1)\n",
    "\n",
    "    detection_model.fit(x = input_train, \n",
    "                   y = y_train, \n",
    "                   epochs = 20, \n",
    "                   batch_size = 300,\n",
    "                   verbose = 1,\n",
    "                   validation_data=(input_test, y_test))\n",
    "\n",
    "detection_model.save('./detection_model_A.h5')\n",
    "detection_model.save_weights('./detection_model_weights_A.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection + Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
