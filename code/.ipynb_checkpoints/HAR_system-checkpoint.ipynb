{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR system - Lincetto Riccardo, Drago Matteo\n",
    "This notebook runs:\n",
    "- Classification with null class (One Shot classification);\n",
    "- Binary classification for activity detection (Two Steps - detection);\n",
    "- Classification without null class (Two Steps - classification);\n",
    "- Cascade of the last two methods.\n",
    "\n",
    "The operations performed here are very similar to those execute in 'main.py', with the exception that here the program is executed for specified user and model.\n",
    "\n",
    "## Notebook setup\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- task: choose \"A\" for locomotion classification or \"B\" for gesture recognition;\n",
    "- model_name: choose between \"Convolutional\", \"Convolutional1DRecurrent\", \"Convolutional2DRecurrent\" and \"ConvolutionalDeepRecurrent\";\n",
    "- data_folder: directory name where '.mat' files are stored;\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window;\n",
    "- GPU: boolean flag indicatin wheter GPU is present on the machine that executes the code;\n",
    "- epochs: number of complete sweeps of the data signals during training;\n",
    "- batch_size: number of forward propagations in the networks between consecutives backpropagations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "task = \"A\"\n",
    "model_name = \"Convolutional\"\n",
    "data_folder = \"./data/full/\"\n",
    "window_size = 15\n",
    "stride = 5\n",
    "GPU = True\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the useful functions are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riccardo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "import models\n",
    "import utils\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from 'main.py', all results saved from this notebook are going to be stored in a dedicated folder: './data/notebook/'. For proper execution, this folder needs first to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(os.path.exists(\"./data\")):\n",
    "    os.mkdir(\"./data\")\n",
    "if not(os.path.exists(\"./data/notebook\")):\n",
    "    os.mkdir(\"./data/notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If task A is selected, calssifications in the following notebook are based on the labels of column 0; if instead it's task B, column 6 labels are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A uses labels column 0\n"
     ]
    }
   ],
   "source": [
    "if task == \"A\":\n",
    "    label = 0\n",
    "    classes = [\"Inactive\",\"Stand\",\"Walk\",\"Sit\",\"Lie\"]\n",
    "elif task == \"B\":\n",
    "    label = 6\n",
    "    classes = [\"Close Dishwasher\",\"Close Drawer 3\",\"Close Drawer 2\",\"Inactive\",\"Close Door 1\",\"Close Door 2\",\"Close Drawer 1\",\"Close Fridge\",\"Toggle Switch\",\"Open Dishwasher\",\"Open Drawer 3\",\"Open Drawer 2\",\"Open Door 1\",\"Open Door 2\",\"Open Drawer 1\",\"Open Fridge\",\"Drink from Cup\",\"Clean Table\"]\n",
    "else:\n",
    "    print(\"Error: invalid task.\")\n",
    "print(\"Task\", task, \"uses labels column\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with null class: One Shot classification\n",
    "Here classification is performed considering inactivity as a class, alongside with the others. In the case of locomotion classification (task A), this becomes a 5-class problem, while in the case of gesture recognition (task B) the classes become 18. In the following cell are perfomed in order:\n",
    "- preprocessing;\n",
    "- model selection;\n",
    "- model compilation;\n",
    "- training.\n",
    "\n",
    "Note that in case \"Convolutional2DRecurrent\" is the model selected, then the preprocessed data need to be reshaped, adding one dimension; this is automatically done by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: A_Convolutional_OS_2 \n",
      "Location: ./data/notebook/A_Convolutional_OS_2.hdf5 \n",
      "\n",
      "Train on 29158 samples, validate on 11541 samples\n",
      "Epoch 1/10\n",
      "18304/29158 [=================>............] - ETA: 5s - loss: 0.9858 - acc: 0.7551"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=data_folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)\n",
    "\n",
    "# model selection\n",
    "if model_name == \"Convolutional\":\n",
    "    model = models.Convolutional((window_size, n_features), n_classes, print_info=False)\n",
    "elif model_name == \"Convolutional1DRecurrent\":\n",
    "    model = models.Convolutional1DRecurrent((window_size, n_features), n_classes, GPU=GPU, print_info=False)\n",
    "elif model_name == \"Convolutional2DRecurrent\":\n",
    "    model = models.Convolutional2DRecurrent((window_size, n_features, 1), n_classes, GPU=GPU, print_info=False)\n",
    "    # reshaping for 2D convolutional model\n",
    "    X_train = X_train.reshape(X_train.shape[0], window_size, n_features, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], window_size, n_features, 1)\n",
    "elif model_name == \"ConvolutionalDeepRecurrent\":\n",
    "    model = models.ConvolutionalDeepRecurrent((window_size, n_features), n_classes, GPU=GPU, print_info=False)\n",
    "else:\n",
    "    print(\"Model not found.\")\n",
    "\n",
    "# model compilation\n",
    "model.compile(optimizer = Adam(lr=0.001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "save_model_name = task + \"_\" + model_name + \"_OS_\" + str(subject)\n",
    "filepath = './data/notebook/'+save_model_name+'.hdf5'\n",
    "print(\"Model:\", save_model_name, \"\\nLocation:\", filepath, \"\\n\")\n",
    "\n",
    "# training\n",
    "checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "model.fit(x = X_train, \n",
    "        y = to_categorical(Y_train), \n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        verbose = 1,\n",
    "        validation_data=(X_test, to_categorical(Y_test)),\n",
    "        callbacks=[checkpointer, lr_reducer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last model\n",
    "Y_pred = model.predict_classes(X_test)\n",
    "score_OS = f1_score(Y_test, Y_pred, average='weighted')\n",
    "\n",
    "# best model\n",
    "model_best = load_model(filepath)\n",
    "Y_pred_best = model_best.predict_classes(X_test)\n",
    "score_OS_best = f1_score(Y_test, Y_pred_best, average='weighted')\n",
    "\n",
    "# keep highest f1-score\n",
    "if score_OS_best > score_OS:\n",
    "    score_OS = score_OS_best\n",
    "    print(\"Results for best \"+ save_model_name + \":\\n\\n\", classification_report(Y_test, Y_pred_best))\n",
    "    # confusion matrix\n",
    "    cnf_matrix = confusion_matrix(Y_test, Y_pred_best)\n",
    "    # save for future use\n",
    "    Y_true = Y_test\n",
    "    Y_OS = Y_pred_best\n",
    "else:\n",
    "    print(\"Results for last \"+ save_model_name + \":\\n\\n\", classification_report(Y_test, Y_pred))\n",
    "    # confusion matrix\n",
    "    cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "    # save for future use\n",
    "    Y_true = Y_test\n",
    "    Y_OS = Y_pred\n",
    "    \n",
    "# print confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "sns.set_style(\"dark\")\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=classes, title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification for activity detection\n",
    "Here all the \"activity\" classes are labelled with 1, while the inactivity class remains labelled with 0. As before, in the following cell are perfomed in order:\n",
    "- preprocessing;\n",
    "- model selection;\n",
    "- model compilation;\n",
    "- training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=data_folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=True,\n",
    "                                                                                                null_class=True,\n",
    "                                                                                                print_info=False)\n",
    "\n",
    "# model selection\n",
    "if model_name == \"Convolutional\":\n",
    "    model = models.Convolutional((window_size, n_features), n_classes, print_info=False)\n",
    "elif model_name == \"Convolutional1DRecurrent\":\n",
    "    model = models.Convolutional1DRecurrent((window_size, n_features), n_classes, GPU=GPU, print_info=False)\n",
    "elif model_name == \"Convolutional2DRecurrent\":\n",
    "    model = models.Convolutional2DRecurrent((window_size, n_features, 1), n_classes, GPU=GPU, print_info=False)\n",
    "    # reshaping for 2D convolutional model\n",
    "    X_train = X_train.reshape(X_train.shape[0], window_size, n_features, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], window_size, n_features, 1)\n",
    "elif model_name == \"ConvolutionalDeepRecurrent\":\n",
    "    model = models.ConvolutionalDeepRecurrent((window_size, n_features), n_classes, GPU=GPU, print_info=False)\n",
    "else:\n",
    "    print(\"Model not found.\")\n",
    "    \n",
    "# model compilation\n",
    "model.compile(optimizer = Adam(lr=0.001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "save_model_name = task + \"_\" + model_name + \"_TSD_\" + str(subject)\n",
    "filepath = './data/notebook/'+save_model_name+'.hdf5'\n",
    "print(\"Model:\", save_model_name, \"\\nLocation:\", filepath, \"\\n\")\n",
    "\n",
    "# training\n",
    "checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "model.fit(x = X_train, \n",
    "        y = to_categorical(Y_train), \n",
    "        epochs = epochs, \n",
    "        batch_size = batch_size,\n",
    "        verbose = 1,\n",
    "        validation_data=(X_test, to_categorical(Y_test)),\n",
    "        callbacks=[checkpointer, lr_reducer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last model\n",
    "Y_pred = model.predict_classes(X_test)\n",
    "score_TSD = f1_score(Y_test, Y_pred, average='weighted')\n",
    "\n",
    "# best model\n",
    "model_best = load_model(filepath)\n",
    "Y_pred_best = model_best.predict_classes(X_test)\n",
    "score_TSD_best = f1_score(Y_test, Y_pred_best, average='weighted')\n",
    "\n",
    "# keep highest f1-score\n",
    "if score_TSD_best > score_TSD:\n",
    "    score_TSD = score_TSD_best\n",
    "    print(\"Results for best \"+ save_model_name + \":\\n\\n\", classification_report(Y_test, Y_pred_best))\n",
    "    # confusion matrix\n",
    "    cnf_matrix = confusion_matrix(Y_test, Y_pred_best)\n",
    "    # save for future use\n",
    "    Y_det = Y_pred_best\n",
    "else:\n",
    "    print(\"Results for last \"+ save_model_name + \":\\n\\n\", classification_report(Y_test, Y_pred))\n",
    "    # confusion matrix\n",
    "    cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "    # save for future use\n",
    "    Y_det = Y_pred\n",
    "    \n",
    "# print confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "sns.set_style(\"dark\")\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=[\"Inactive\",\"Active\"], title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification without null class: Two Steps - classification\n",
    "Here all the \"activity\" classes are kept while the inactivity class is discarded during preprocessing. Again, in the following cell are perfomed in order:\n",
    "- preprocessing;\n",
    "- model selection;\n",
    "- model compilation;\n",
    "- training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X_train, Y_train, X_test, Y_test, n_features, n_classes, class_weights = preprocessing.loadData(subject=subject,\n",
    "                                                                                                label=label,\n",
    "                                                                                                folder=data_folder,\n",
    "                                                                                                window_size=window_size,\n",
    "                                                                                                stride=stride,\n",
    "                                                                                                make_binary=False,\n",
    "                                                                                                null_class=False,\n",
    "                                                                                                print_info=False)\n",
    "\n",
    "# model selection\n",
    "if model_name == \"Convolutional\":\n",
    "    model = models.Convolutional((window_size, n_features), n_classes, print_info=False)\n",
    "elif model_name == \"Convolutional1DRecurrent\":\n",
    "    model = models.Convolutional1DRecurrent((window_size, n_features), n_classes, GPU=GPU, print_info=False)\n",
    "elif model_name == \"Convolutional2DRecurrent\":\n",
    "    model = models.Convolutional2DRecurrent((window_size, n_features, 1), n_classes, GPU=GPU, print_info=False)\n",
    "    # reshaping for 2D convolutional model\n",
    "    X_train = X_train.reshape(X_train.shape[0], window_size, n_features, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], window_size, n_features, 1)\n",
    "elif model_name == \"ConvolutionalDeepRecurrent\":\n",
    "    model = models.ConvolutionalDeepRecurrent((window_size, n_features), n_classes, GPU=GPU, print_info=False)\n",
    "else:\n",
    "    print(\"Model not found.\")\n",
    "\n",
    "# model compilation\n",
    "model.compile(optimizer = Adam(lr=0.001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "save_model_name = task + \"_\" + model_name + \"_TSC_\" + str(subject)\n",
    "filepath = './data/notebook/'+save_model_name+'.hdf5'\n",
    "print(\"Model:\", save_model_name, \"\\nLocation:\", filepath, \"\\n\")\n",
    "\n",
    "# training\n",
    "checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "model.fit(x = X_train, \n",
    "        y = to_categorical(Y_train), \n",
    "        epochs = epochs, \n",
    "        batch_size = batch_size,\n",
    "        verbose = 1,\n",
    "        validation_data=(X_test, to_categorical(Y_test)),\n",
    "        callbacks=[checkpointer, lr_reducer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last model\n",
    "Y_pred = model.predict_classes(X_test)\n",
    "score_TSC = f1_score(Y_test, Y_pred, average='weighted')\n",
    "\n",
    "# best model\n",
    "model_best = load_model(filepath)\n",
    "Y_pred_best = model_best.predict_classes(X_test)\n",
    "score_TSC_best = f1_score(Y_test, Y_pred_best, average='weighted')\n",
    "\n",
    "# keep highest f1-score\n",
    "if score_TSC_best > score_TSC:\n",
    "    score_TSC = score_TSC_best\n",
    "    print(\"Results for best \"+ save_model_name + \":\\n\\n\", classification_report(Y_test, Y_pred_best))\n",
    "    # confusion matrix\n",
    "    cnf_matrix = confusion_matrix(Y_test, Y_pred_best)\n",
    "else:\n",
    "    print(\"Results for last \"+ save_model_name + \":\\n\\n\", classification_report(Y_test, Y_pred))\n",
    "    # confusion matrix\n",
    "    cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "    \n",
    "# print confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "sns.set_style(\"dark\")\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=classes[1:], title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purposes, the best F1-scores of the three classifications performed above are saved in the variables: 'score_OS', 'score_TSD', 'score_TSC'. These will be plotted later, to be compared with the cascade approach implemented next.\n",
    "## Cascade of detection and classification\n",
    "For the evaluation of this approach, the model previously trained are fine: the classification model in particular is trained on the \"correct\" training set, but then makes its prediction on data coming from the detector. For this part we need some results from the previous code:\n",
    "- true labels, which correspond to those obtained with the preprocessing of the One Shot classification model ('Y_true');\n",
    "- predicted labels, by the One Shot classification model ('Y_OS');\n",
    "- detector predictions on test data, which need to be passed to the second stage of the cascade ('Y_det');\n",
    "- classification model, which is the last one that has been used ('model').\n",
    "\n",
    "In the following cell true labels are recalled, the detection model is loaded and used to predict the labels on the test set and, finally, the predictions are used to perform a further classification. Once the predictions are made, they are compared to the true labels: note that to this end, classification predictions need to be inserted in an array containing the detection predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print info on previous predictions\n",
    "print(\"Y_true:  Shape:\", Y_true.shape, \" Labels:\", np.unique(Y_true),\n",
    "     \"\\nY_OS:    Shape:\", Y_OS.shape, \" Labels:\", np.unique(Y_OS),\n",
    "     \"\\nY_det:   Shape:\", Y_det.shape, \" Labels:\", np.unique(Y_det),\n",
    "     \"\\nY_pred:  Shape:\", Y_pred.shape, \"  Labels:\", np.unique(Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that Y_pred, the predictions by the classifier without the null class, should have label values starting from 1, thus a manual correction will be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test set\n",
    "X_test = preprocessing.loadData(subject=subject,\n",
    "                                label=label,\n",
    "                                folder=data_folder,\n",
    "                                window_size=window_size,\n",
    "                                stride=stride,\n",
    "                                make_binary=False,\n",
    "                                null_class=True,\n",
    "                                print_info=True)[2]\n",
    "\n",
    "# load detection model\n",
    "# detector_name = task + \"_\" + model_name + \"_TSD_\" + str(subject)\n",
    "# filepath = './data/notebook/'+save_model_name+'.hdf5'\n",
    "# detector = load_model(filepath)\n",
    "\n",
    "# predict binary classes\n",
    "# Y_det = detector.predict_classes(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a mask is used to select all the windows where activity is detected. Note that this number of windows can be different from the number of non-zero labels (which correspond to the shape of Y_pred, reported above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask\n",
    "mask = (Y_det == 1)\n",
    "print(\"Mask shape:\", mask.shape, \"\\nDetected activities:\", np.sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_windows = X_test[mask, :, :]\n",
    "print(\"New test set has shape:\", activity_windows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification previously trained is now used to predict the classes on the new test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_clas = model_best.predict_classes(activity_windows) + 1\n",
    "print(\"Y_clas:  Shape:\", Y_clas.shape, \" Labels:\", np.unique(Y_clas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions are inserted back in the detector's values, substituting these new labels to the ones with value 1. The locations to be modified are still those flagged by the 'mask'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TS = Y_det\n",
    "Y_TS[mask] = Y_clas\n",
    "print(\"Y_TS:    Shape:\", Y_TS.shape, \" Labels:\", np.unique(Y_TS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_true, Y_TS))\n",
    "cnf_matrix = confusion_matrix(Y_true, Y_TS)\n",
    "np.set_printoptions(precision=2)\n",
    "sns.set_style(\"dark\")\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=classes[1:], title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between the two pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One-Shot weighted f1-score:  \", score_OS)\n",
    "print(\"Two-Steps weighted f1-score: \", f1_score(Y_true, Y_TS, average='weighted'))\n",
    "print(\"With partials:\\n\\t\\tDetection:     \",score_TSB,\"\\n\\t\\tClassification:\", score_TSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
