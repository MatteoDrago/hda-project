{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3\n",
    "\n",
    "This first cell contains the hyper-parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: column of features to be selected to perform activity detection, between [0,6];\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.activations import relu\n",
    "from keras.layers import Conv1D, Conv2D, BatchNormalization, Dropout, LeakyReLU, Flatten, Activation, Dense, MaxPooling1D, MaxPooling2D, LSTM, Reshape, TimeDistributed, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters definition\n",
    "\n",
    "subject = 1\n",
    "folder = \"./data/full/\"\n",
    "label_col = 0     # default for task A\n",
    "window_size = 64\n",
    "stride = 3\n",
    "null_class = True\n",
    "\n",
    "if(null_class):\n",
    "    n_classes = 5\n",
    "    classes = [0,1,2,4,5]\n",
    "else:\n",
    "    n_classes = 4\n",
    "    classes = [1,2,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Loading and Preprocessing\n",
    "\n",
    "### Dataset Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session shapes:\n",
      "ADL1:   (45810, 110)\n",
      "ADL2:   (28996, 110)\n",
      "ADL3:   (30167, 110)\n",
      "ADL4:   (30228, 110)\n",
      "ADL5:   (27308, 110)\n",
      "Drill:  (52152, 110)\n",
      "\n",
      "Training samples:  157125 \n",
      "Test samples:       57536 \n",
      "Features:             110\n",
      "\n",
      "Classes in training set:  5 \n",
      "Classes in test set:      5\n",
      "Training set:\n",
      "<class 'numpy.ndarray'> (52354, 64, 110) <class 'numpy.ndarray'> (52354, 5)\n",
      "\n",
      "Features have shape:  (52354, 64, 110) \n",
      "Labels have shape:    (52354, 5) \n",
      "Fraction of labels:   [0.10988654 0.41987241 0.27476411 0.17119991 0.02427704]\n",
      "\n",
      "Test set:\n",
      "<class 'numpy.ndarray'> (19157, 64, 110) <class 'numpy.ndarray'> (19157, 5)\n",
      "\n",
      "Features have shape:  (19157, 64, 110) \n",
      "Labels have shape:    (19157, 5) \n",
      "Fraction of labels:   [0.17742862 0.34337318 0.20290233 0.23771989 0.03857598]\n"
     ]
    }
   ],
   "source": [
    "# import all sessions for a subject\n",
    "(data1, data2, data3, data4, data5, data6) = utils.loadData(subject, folder=folder)\n",
    "\n",
    "# create training set and test set\n",
    "X_train = np.concatenate((data1['features_interp'],\\\n",
    "                          data2['features_interp'],\\\n",
    "                          data3['features_interp'],\\\n",
    "                          data6['features_interp']), axis=0)\n",
    "\n",
    "Y_train = np.concatenate((data1['labels_cut'][:,label_col],\\\n",
    "                          data2['labels_cut'][:,label_col],\\\n",
    "                          data3['labels_cut'][:,label_col],\\\n",
    "                          data6['labels_cut'][:,label_col]), axis=0)\n",
    "\n",
    "X_test = np.concatenate((data4['features_interp'],\\\n",
    "                         data5['features_interp']), axis=0)\n",
    "\n",
    "Y_test = np.concatenate((data4['labels_cut'][:,label_col],\\\n",
    "                         data5['labels_cut'][:,label_col]))\n",
    "\n",
    "features = X_test.shape[1]\n",
    "print(\"\\nTraining samples: \", X_train.shape[0],\\\n",
    "      \"\\nTest samples:      \", X_test.shape[0],\\\n",
    "      \"\\nFeatures:            \", features)\n",
    "\n",
    "# decision to overcome the problem of entire missing columns\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "# features normalization\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train =scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# switch to one hot encoded labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "Y_train_oh = onehot_encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "Y_test_oh = onehot_encoder.fit_transform(Y_test.reshape(-1, 1))\n",
    "print(\"\\nClasses in training set: \", Y_train_oh.shape[1],\\\n",
    "      \"\\nClasses in test set:     \", Y_test_oh.shape[1])\n",
    "\n",
    "print(\"Training set:\")\n",
    "X_train_s, Y_train_s = utils.prepareData(X_train, Y_train_oh, window_size, stride, shuffle=False, null_class = null_class)\n",
    "print(\"\\nTest set:\")\n",
    "X_test_s, Y_test_s = utils.prepareData(X_test, Y_test_oh, window_size, stride, shuffle=False, null_class = null_class)\n",
    "# add bars plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Deep CNN and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelDeep2D(input_shape, classes, withSoftmax = True):\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns: \n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "  \n",
    "    model.add(BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(filters = 50,\n",
    "                    kernel_size = (11,1),\n",
    "                    activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "    model.add(Reshape((27,5500)))\n",
    "    \n",
    "    model.add(LSTM(100,\n",
    "                  return_sequences=True))\n",
    "    \n",
    "    model.add(LSTM(100))\n",
    "    \n",
    "    model.add(Dense(512,activation = 'relu'))\n",
    "    \n",
    "    if withSoftmax:\n",
    "        model.add(Dense(classes, activation = 'softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_4 (Batch (None, 64, 110, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 54, 110, 50)       600       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 27, 110, 50)       0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 27, 5500)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 27, 100)           2240400   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               51712     \n",
      "=================================================================\n",
      "Total params: 2,373,116\n",
      "Trainable params: 2,373,114\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_temp = ModelDeep2D((window_size,features,1), n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt4 = Adam(lr=0.01)\n",
    "model_temp.compile(optimizer = opt4, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# need to reshape in order to fit to the new 2D model\n",
    "X_train = X_train_s.reshape(X_train_s.shape[0], window_size, features, 1)\n",
    "X_test = X_test_s.reshape(X_test_s.shape[0], window_size, features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (512,) but got array with shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-7a16d8ef6ce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                validation_data=(X_test, X_test))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (512,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "model_temp.fit(x = X_train, \n",
    "               y = Y_train_s, \n",
    "               epochs = 25, \n",
    "               batch_size = 200,\n",
    "               verbose = 1,\n",
    "               validation_data=(X_test, Y_test_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and labels\n",
    "print('Loading the training and testing data ...')\n",
    "trainingShape = X_train.shape\n",
    "testingShape = X_test.shape\n",
    "assert trainingShape[1] == testingShape[1] # Window size\n",
    "assert trainingShape[2] == testingShape[2] # Nb of sensors\n",
    "\n",
    "nbTrainingExamples = trainingShape[0]\n",
    "assert len(y_train) == nbTrainingExamples\n",
    "print('   %d training examples loaded' % (nbTrainingExamples))\n",
    "\n",
    "nbTestingExamples = testingShape[0]\n",
    "assert len(y_test) == nbTestingExamples\n",
    "print('   %d testing examples loaded' % (nbTestingExamples))\n",
    "\n",
    "# Allocate the feature arrays\n",
    "featureSize = 512\n",
    "trainingDnnFeatures = np.empty((nbTrainingExamples,featureSize),dtype=np.float32)\n",
    "testingDnnFeatures = np.empty((nbTestingExamples,featureSize),dtype=np.float32)\n",
    "\n",
    "print('Computing DNN features on the training set...')\n",
    "idx = 0\n",
    "while idx < nbTrainingExamples:\n",
    "    if idx + batchSize < nbTrainingExamples:\n",
    "        endIdx = idx+batchSize\n",
    "        size = batchSize\n",
    "    else:\n",
    "        endIdx = nbTrainingExamples\n",
    "        size = nbTrainingExamples-idx\n",
    "    predictions = model.predict(x_train[idx:endIdx],batch_size=size)\n",
    "    trainingDnnFeatures[idx:endIdx] = predictions\n",
    "    idx += batchSize\n",
    "\n",
    "print('Computing DNN features on the testing set...')\n",
    "idx = 0\n",
    "while idx < nbTestingExamples:\n",
    "    if idx + batchSize < nbTestingExamples:\n",
    "        endIdx = idx+batchSize\n",
    "        size = batchSize\n",
    "    else:\n",
    "        endIdx = nbTestingExamples\n",
    "        size = nbTestingExamples-idx\n",
    "    predictions = model.predict(x_test[idx:endIdx],batch_size=size)\n",
    "    testingDnnFeatures[idx:endIdx] = predictions\n",
    "    idx += batchSize\n",
    "\n",
    "# Save features and labels\n",
    "#print('Saving results ...')\n",
    "\n",
    "#np.save('/dnnFeatures_training.npy',trainingDnnFeatures)\n",
    "#np.save('/dnnLabels_training.npy',y_train)\n",
    "#np.save('/dnnFeatures_testing.npy',testingDnnFeatures)\n",
    "#np.save('/dnnLabels_testing.npy',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trainingData = np.load(trainingDataPath)\n",
    "#trainingLabels = np.load(trainingLabelsPath)\n",
    "#testingData = np.load(testingDataPath)\n",
    "#testingLabels = np.load(testingLabelsPath)\n",
    "\n",
    "# Train the linear SVM model\n",
    "\n",
    "C = [2**(-6)]\n",
    "\n",
    "for idx in range(len(C)):\n",
    "    print('Training the model with C = %.4f' % (C[idx]))\n",
    "    classifier = LinearSVC(C=C[idx])\n",
    "    classifier.fit(trainingDnnFeatures,Y_train_s)\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    print('   Evaluating the model')\n",
    "    estimatedLabels = classifier.predict(testingDnnFeatures)\n",
    "\n",
    "    # Compute the accuracy, weighted F1-score and average F1-score\n",
    "    accuracy = utils.accuracy_score(Y_test_s,estimatedLabels)\n",
    "    weightedF1 = utils.f1_score(Y_test_s,estimatedLabels,average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print('   Test accuracy = %.2f %%' % (accuracy*100))\n",
    "    print('   Weighted F1-score = %.4f' % (weightedF1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_pred_s = model_temp.predict(X_test)\n",
    "\n",
    "# print results\n",
    "#reverse the one-ot encoder procedure\n",
    "#Y_test_hard = np.argmax(Y_test_s, axis=1)\n",
    "#Y_pred_hard = np.argmax(Y_pred_s, axis=1)\n",
    "\n",
    "#print(\"F1-measure: \", utils.f1_score(Y_test_hard, Y_pred_hard, average='weighted'))\n",
    "#print(\"AUC w.r. to each class: \", utils.AUC(Y_test_s, Y_pred_s, 5))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "#cnf_matrix = utils.confusion_matrix(Y_test_hard, Y_pred_hard)\n",
    "#np.set_printoptions(precision=2)\n",
    "\n",
    "#plt.figure()\n",
    "#utils.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "#                      title='Confusion matrix, without normalization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
