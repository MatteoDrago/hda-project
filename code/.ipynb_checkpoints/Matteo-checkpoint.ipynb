{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3\n",
    "\n",
    "This first cell contains the hyper-parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: column of features to be selected to perform activity detection, between [0,6];\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.activations import relu\n",
    "from keras.layers import Conv1D, Conv2D, BatchNormalization, Dropout, LeakyReLU, Flatten, Activation, Dense, MaxPooling1D, MaxPooling2D, LSTM, Reshape, TimeDistributed, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters definition\n",
    "\n",
    "subject = 1\n",
    "folder = \"./data/full/\"\n",
    "label_col = 0     # default for task A\n",
    "window_size = 64\n",
    "stride = 3\n",
    "null_class = True\n",
    "\n",
    "if(null_class):\n",
    "    n_classes = 5\n",
    "    classes = [0,1,2,4,5]\n",
    "else:\n",
    "    n_classes = 4\n",
    "    classes = [1,2,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Loading and Preprocessing\n",
    "\n",
    "### Dataset Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session shapes:\n",
      "ADL1:   (45810, 110)\n",
      "ADL2:   (28996, 110)\n",
      "ADL3:   (30167, 110)\n",
      "ADL4:   (30228, 110)\n",
      "ADL5:   (27308, 110)\n",
      "Drill:  (52152, 110)\n",
      "\n",
      "Training samples:  157125 \n",
      "Test samples:       57536 \n",
      "Features:             110\n",
      "\n",
      "Classes in training set:  5 \n",
      "Classes in test set:      5\n",
      "Training set:\n",
      "<class 'numpy.ndarray'> (52354, 64, 110) <class 'numpy.ndarray'> (52354, 5)\n",
      "\n",
      "Features have shape:  (52354, 64, 110) \n",
      "Labels have shape:    (52354, 5) \n",
      "Fraction of labels:   [0.10988654 0.41987241 0.27476411 0.17119991 0.02427704]\n",
      "\n",
      "Test set:\n",
      "<class 'numpy.ndarray'> (19157, 64, 110) <class 'numpy.ndarray'> (19157, 5)\n",
      "\n",
      "Features have shape:  (19157, 64, 110) \n",
      "Labels have shape:    (19157, 5) \n",
      "Fraction of labels:   [0.17742862 0.34337318 0.20290233 0.23771989 0.03857598]\n"
     ]
    }
   ],
   "source": [
    "# import all sessions for a subject\n",
    "(data1, data2, data3, data4, data5, data6) = utils.loadData(subject, folder=folder)\n",
    "\n",
    "# create training set and test set\n",
    "X_train = np.concatenate((data1['features_interp'],\\\n",
    "                          data2['features_interp'],\\\n",
    "                          data3['features_interp'],\\\n",
    "                          data6['features_interp']), axis=0)\n",
    "\n",
    "Y_train = np.concatenate((data1['labels_cut'][:,label_col],\\\n",
    "                          data2['labels_cut'][:,label_col],\\\n",
    "                          data3['labels_cut'][:,label_col],\\\n",
    "                          data6['labels_cut'][:,label_col]), axis=0)\n",
    "\n",
    "X_test = np.concatenate((data4['features_interp'],\\\n",
    "                         data5['features_interp']), axis=0)\n",
    "\n",
    "Y_test = np.concatenate((data4['labels_cut'][:,label_col],\\\n",
    "                         data5['labels_cut'][:,label_col]))\n",
    "\n",
    "features = X_test.shape[1]\n",
    "print(\"\\nTraining samples: \", X_train.shape[0],\\\n",
    "      \"\\nTest samples:      \", X_test.shape[0],\\\n",
    "      \"\\nFeatures:            \", features)\n",
    "\n",
    "# decision to overcome the problem of entire missing columns\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "# features normalization\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train =scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# switch to one hot encoded labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "Y_train_oh = onehot_encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "Y_test_oh = onehot_encoder.fit_transform(Y_test.reshape(-1, 1))\n",
    "print(\"\\nClasses in training set: \", Y_train_oh.shape[1],\\\n",
    "      \"\\nClasses in test set:     \", Y_test_oh.shape[1])\n",
    "\n",
    "print(\"Training set:\")\n",
    "X_train_s, Y_train_s = utils.prepareData(X_train, Y_train_oh, window_size, stride, shuffle=False, null_class = null_class)\n",
    "print(\"\\nTest set:\")\n",
    "X_test_s, Y_test_s = utils.prepareData(X_test, Y_test_oh, window_size, stride, shuffle=False, null_class = null_class)\n",
    "# add bars plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Deep CNN and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelDeep2D(input_shape, classes):\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "\n",
    "    Returns: \n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "  \n",
    "    model.add(BatchNormalization(input_shape = input_shape))\n",
    "    model.add(Conv2D(filters = 50,\n",
    "                    kernel_size = (11,1),\n",
    "                    activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "    model.add(Reshape((27,5500)))\n",
    "    \n",
    "    model.add(LSTM(100,\n",
    "                  return_sequences=True))\n",
    "    \n",
    "    model.add(LSTM(100))\n",
    "    \n",
    "    model.add(Dense(512,activation = 'relu'))\n",
    "    \n",
    "    #model.add(Dense(classes, activation = 'softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 64, 110, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 54, 110, 50)       600       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 110, 50)       0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 27, 5500)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 27, 100)           2240400   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               51712     \n",
      "=================================================================\n",
      "Total params: 2,373,116\n",
      "Trainable params: 2,373,114\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_temp = ModelDeep2D((window_size,features,1), n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt4 = Adam(lr=0.01)\n",
    "model_temp.compile(optimizer = opt4, loss = \"categorical_crossentropy\", metrics = [\"accuracy\",fmeasure])\n",
    "\n",
    "# need to reshape in order to fit to the new 2D model\n",
    "X_train = X_train_s.reshape(X_train_s.shape[0], window_size, features, 1)\n",
    "X_test = X_test_s.reshape(X_test_s.shape[0], window_size, features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temp.fit(x = X_train, \n",
    "               y = Y_train_s, \n",
    "               epochs = 25, \n",
    "               batch_size = 200,\n",
    "               verbose = 1,\n",
    "               validation_data=(X_test, Y_test_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and labels\n",
    "print('Loading the training and testing data ...')\n",
    "trainingShape = X_train.shape\n",
    "testingShape = X_test.shape\n",
    "assert trainingShape[1] == testingShape[1] # Window size\n",
    "assert trainingShape[2] == testingShape[2] # Nb of sensors\n",
    "\n",
    "nbTrainingExamples = trainingShape[0]\n",
    "assert len(y_train) == nbTrainingExamples\n",
    "print('   %d training examples loaded' % (nbTrainingExamples))\n",
    "\n",
    "nbTestingExamples = testingShape[0]\n",
    "assert len(y_test) == nbTestingExamples\n",
    "print('   %d testing examples loaded' % (nbTestingExamples))\n",
    "\n",
    "# Allocate the feature arrays\n",
    "featureSize = 512\n",
    "trainingDnnFeatures = np.empty((nbTrainingExamples,featureSize),dtype=np.float32)\n",
    "testingDnnFeatures = np.empty((nbTestingExamples,featureSize),dtype=np.float32)\n",
    "\n",
    "print('Computing DNN features on the training set...')\n",
    "idx = 0\n",
    "while idx < nbTrainingExamples:\n",
    "    if idx + batchSize < nbTrainingExamples:\n",
    "      endIdx = idx+batchSize\n",
    "      size = batchSize\n",
    "    else:\n",
    "      endIdx = nbTrainingExamples\n",
    "      size = nbTrainingExamples-idx\n",
    "    predictions = model.predict(x_train[idx:endIdx],batch_size=size)\n",
    "    trainingDnnFeatures[idx:endIdx] = predictions\n",
    "    idx += batchSize\n",
    "\n",
    "print('Computing DNN features on the testing set...')\n",
    "idx = 0\n",
    "while idx < nbTestingExamples:\n",
    "    if idx + batchSize < nbTestingExamples:\n",
    "      endIdx = idx+batchSize\n",
    "      size = batchSize\n",
    "    else:\n",
    "      endIdx = nbTestingExamples\n",
    "      size = nbTestingExamples-idx\n",
    "    predictions = model.predict(x_test[idx:endIdx],batch_size=size)\n",
    "    testingDnnFeatures[idx:endIdx] = predictions\n",
    "    idx += batchSize\n",
    "\n",
    "# Save features and labels\n",
    "#print('Saving results ...')\n",
    "\n",
    "#np.save('/dnnFeatures_training.npy',trainingDnnFeatures)\n",
    "#np.save('/dnnLabels_training.npy',y_train)\n",
    "#np.save('/dnnFeatures_testing.npy',testingDnnFeatures)\n",
    "#np.save('/dnnLabels_testing.npy',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trainingData = np.load(trainingDataPath)\n",
    "#trainingLabels = np.load(trainingLabelsPath)\n",
    "#testingData = np.load(testingDataPath)\n",
    "#testingLabels = np.load(testingLabelsPath)\n",
    "\n",
    "# Train the linear SVM model\n",
    "\n",
    "C = [2**(-6)]\n",
    "\n",
    "for idx in range(len(C)):\n",
    "    print('Training the model with C = %.4f' % (C[idx]))\n",
    "    classifier = LinearSVC(C=C[idx])\n",
    "    classifier.fit(trainingDnnFeatures,Y_train_s)\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    print('   Evaluating the model')\n",
    "    estimatedLabels = classifier.predict(testingDnnFeatures)\n",
    "\n",
    "    # Compute the accuracy, weighted F1-score and average F1-score\n",
    "    accuracy = utils.accuracy_score(Y_test_s,estimatedLabels)\n",
    "    weightedF1 = utils.f1_score(Y_test_s,estimatedLabels,average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print('   Test accuracy = %.2f %%' % (accuracy*100))\n",
    "    print('   Weighted F1-score = %.4f' % (weightedF1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_pred_s = model_temp.predict(X_test)\n",
    "\n",
    "# print results\n",
    "#reverse the one-ot encoder procedure\n",
    "#Y_test_hard = np.argmax(Y_test_s, axis=1)\n",
    "#Y_pred_hard = np.argmax(Y_pred_s, axis=1)\n",
    "\n",
    "#print(\"F1-measure: \", utils.f1_score(Y_test_hard, Y_pred_hard, average='weighted'))\n",
    "#print(\"AUC w.r. to each class: \", utils.AUC(Y_test_s, Y_pred_s, 5))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "#cnf_matrix = utils.confusion_matrix(Y_test_hard, Y_pred_hard)\n",
    "#np.set_printoptions(precision=2)\n",
    "\n",
    "#plt.figure()\n",
    "#utils.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "#                      title='Confusion matrix, without normalization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
