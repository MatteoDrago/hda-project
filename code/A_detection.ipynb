{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import deeplearning\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.activations import relu\n",
    "from keras.layers import Conv2D, BatchNormalization, Dropout, LeakyReLU, Flatten, Activation, Dense, MaxPooling2D, LSTM, Reshape\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the hyper-parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: column of features to be selected to perform activity detection, between [0,6]:\n",
    "\n",
    "|  Label |  Feature |\n",
    "|:-:     |:-:|\n",
    "|  0     | Locomotion (TASK A)  |\n",
    "|  1     | High Level Activity |\n",
    "|  2     | Low Level Left Arm  |\n",
    "|  3     | Low Level Left Arm Object  |\n",
    "|  4     | Low Level Right Arm  |\n",
    "|  5     | Low Level Right Arm Object  |\n",
    "|  6     | Medium Level Both Arms (TASK B2) |\n",
    "\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window.\n",
    "\n",
    "The size of the temporal window seems to be fundamental in order to get a more specific and powerful model; of course the choice of the step lenght between consequent windows has to be consistent and to make sense. Thinking about a real-time situation, as long as we collect data we can use a sliding window of real-time samples; in this way, it is reasonable to use also a small value for the stride. Another important reason behind the choice of the value of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [1,2,3,4]\n",
    "folder = \"./data/full/\"\n",
    "#folder = \"/floyd/input/hdadataset/full/\" # To be used with FloydHub\n",
    "label = 0     # default for task A\n",
    "window_size = 64\n",
    "stride = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection\n",
    "\n",
    "The following section consists on the first part of the structure; our idea is to treat separately the detection of the movement (i.e. the _null class_) and the movement classication itself.\n",
    "\n",
    "The steps that we take are the following: first we set all the labels different from 0 to 1, making the problem binary; then, we build a suitable network that can spot the movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation and input reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 110 #number of features taken into consideration for the solution of the problem\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 64, 110, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 54, 108, 50)       1700      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 108, 50)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 27, 5400)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 27, 20)            433680    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               10752     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 450,442\n",
      "Trainable params: 450,440\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "detection_model = deeplearning.MotionDetection((window_size,n_features,1), n_classes)\n",
    "detection_model.summary() # model visualization\n",
    "\n",
    "detection_model.compile(optimizer = Adam(lr=0.01), \n",
    "                   loss = \"categorical_crossentropy\", \n",
    "                   metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "\n",
    "After the training procedure, the model will be saved on the local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going for USER  1\n",
      "Training samples:  157125 \n",
      "Test samples:       57536 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (52354, 64, 110) \n",
      "Dataset of Labels have shape:    (52354, 2) \n",
      "Fraction of labels:   [0.10986744 0.89013256]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (19157, 64, 110) \n",
      "Dataset of Labels have shape:    (19157, 2) \n",
      "Fraction of labels:   [0.17737642 0.82262358]\n",
      "Train on 52354 samples, validate on 19157 samples\n",
      "Epoch 1/20\n",
      "52354/52354 [==============================] - 76s 1ms/step - loss: 0.1091 - acc: 0.9629 - val_loss: 0.2130 - val_acc: 0.9386\n",
      "Epoch 2/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0602 - acc: 0.9776 - val_loss: 0.1112 - val_acc: 0.9602\n",
      "Epoch 3/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0569 - acc: 0.9809 - val_loss: 0.1363 - val_acc: 0.9592\n",
      "Epoch 4/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0372 - acc: 0.9876 - val_loss: 0.1022 - val_acc: 0.9692\n",
      "Epoch 5/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0377 - acc: 0.9874 - val_loss: 0.1507 - val_acc: 0.9617\n",
      "Epoch 6/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0294 - acc: 0.9897 - val_loss: 0.1263 - val_acc: 0.9581\n",
      "Epoch 7/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0287 - acc: 0.9911 - val_loss: 0.1839 - val_acc: 0.9583\n",
      "Epoch 8/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0264 - acc: 0.9910 - val_loss: 0.1228 - val_acc: 0.9624\n",
      "Epoch 9/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0269 - acc: 0.9904 - val_loss: 0.1755 - val_acc: 0.9633\n",
      "Epoch 10/20\n",
      "52354/52354 [==============================] - 74s 1ms/step - loss: 0.0406 - acc: 0.9850 - val_loss: 0.2160 - val_acc: 0.9521\n",
      "Epoch 11/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0334 - acc: 0.9879 - val_loss: 0.2229 - val_acc: 0.9527\n",
      "Epoch 12/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0340 - acc: 0.9881 - val_loss: 0.1839 - val_acc: 0.9541\n",
      "Epoch 13/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0300 - acc: 0.9903 - val_loss: 0.1707 - val_acc: 0.9556\n",
      "Epoch 14/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0305 - acc: 0.9887 - val_loss: 0.1264 - val_acc: 0.9619\n",
      "Epoch 15/20\n",
      "52354/52354 [==============================] - 72s 1ms/step - loss: 0.0345 - acc: 0.9880 - val_loss: 0.1689 - val_acc: 0.9581\n",
      "Epoch 16/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0443 - acc: 0.9843 - val_loss: 0.1507 - val_acc: 0.9584\n",
      "Epoch 17/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0436 - acc: 0.9840 - val_loss: 0.1726 - val_acc: 0.9565\n",
      "Epoch 18/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0405 - acc: 0.9862 - val_loss: 0.1839 - val_acc: 0.9578\n",
      "Epoch 19/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0337 - acc: 0.9880 - val_loss: 0.1650 - val_acc: 0.9620\n",
      "Epoch 20/20\n",
      "52354/52354 [==============================] - 73s 1ms/step - loss: 0.0373 - acc: 0.9880 - val_loss: 0.1994 - val_acc: 0.9552\n",
      "Going for USER  2\n",
      "Training samples:  145808 \n",
      "Test samples:       57720 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (48581, 64, 110) \n",
      "Dataset of Labels have shape:    (48581, 2) \n",
      "Fraction of labels:   [0.0901587 0.9098413]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (19219, 64, 110) \n",
      "Dataset of Labels have shape:    (19219, 2) \n",
      "Fraction of labels:   [0.15984182 0.84015818]\n",
      "Train on 48581 samples, validate on 19219 samples\n",
      "Epoch 1/20\n",
      "48581/48581 [==============================] - 69s 1ms/step - loss: 0.1957 - acc: 0.9258 - val_loss: 0.2881 - val_acc: 0.9100\n",
      "Epoch 2/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1397 - acc: 0.9491 - val_loss: 0.2262 - val_acc: 0.9314\n",
      "Epoch 3/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1294 - acc: 0.9538 - val_loss: 0.2008 - val_acc: 0.9301\n",
      "Epoch 4/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1306 - acc: 0.9522 - val_loss: 0.2158 - val_acc: 0.9308\n",
      "Epoch 5/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1257 - acc: 0.9558 - val_loss: 0.2538 - val_acc: 0.9155\n",
      "Epoch 6/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1096 - acc: 0.9629 - val_loss: 0.2206 - val_acc: 0.9315\n",
      "Epoch 7/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1069 - acc: 0.9624 - val_loss: 0.1922 - val_acc: 0.9332\n",
      "Epoch 8/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1003 - acc: 0.9641 - val_loss: 0.1969 - val_acc: 0.9308\n",
      "Epoch 9/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0944 - acc: 0.9682 - val_loss: 0.2093 - val_acc: 0.9373\n",
      "Epoch 10/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0938 - acc: 0.9687 - val_loss: 0.2141 - val_acc: 0.9401\n",
      "Epoch 11/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0950 - acc: 0.9680 - val_loss: 0.1771 - val_acc: 0.9421\n",
      "Epoch 12/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0889 - acc: 0.9709 - val_loss: 0.1817 - val_acc: 0.9377\n",
      "Epoch 13/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0960 - acc: 0.9667 - val_loss: 0.1783 - val_acc: 0.9397\n",
      "Epoch 14/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0958 - acc: 0.9671 - val_loss: 0.2191 - val_acc: 0.9400\n",
      "Epoch 15/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0863 - acc: 0.9710 - val_loss: 0.1759 - val_acc: 0.9428\n",
      "Epoch 16/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0841 - acc: 0.9711 - val_loss: 0.2073 - val_acc: 0.9410\n",
      "Epoch 17/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0909 - acc: 0.9699 - val_loss: 0.2547 - val_acc: 0.9419\n",
      "Epoch 18/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.1010 - acc: 0.9645 - val_loss: 0.2047 - val_acc: 0.9409\n",
      "Epoch 19/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0904 - acc: 0.9680 - val_loss: 0.1994 - val_acc: 0.9399\n",
      "Epoch 20/20\n",
      "48581/48581 [==============================] - 68s 1ms/step - loss: 0.0873 - acc: 0.9696 - val_loss: 0.2027 - val_acc: 0.9407\n",
      "Going for USER  3\n",
      "Training samples:  150098 \n",
      "Test samples:       49005 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (50011, 64, 110) \n",
      "Dataset of Labels have shape:    (50011, 2) \n",
      "Fraction of labels:   [0.0204555 0.9795445]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (16314, 64, 110) \n",
      "Dataset of Labels have shape:    (16314, 2) \n",
      "Fraction of labels:   [0.06497487 0.93502513]\n",
      "Train on 50011 samples, validate on 16314 samples\n",
      "Epoch 1/20\n",
      "50011/50011 [==============================] - 69s 1ms/step - loss: 0.0680 - acc: 0.9799 - val_loss: 0.2191 - val_acc: 0.9442\n",
      "Epoch 2/20\n",
      "50011/50011 [==============================] - 69s 1ms/step - loss: 0.0627 - acc: 0.9838 - val_loss: 0.2814 - val_acc: 0.9273\n",
      "Epoch 3/20\n",
      "50011/50011 [==============================] - 70s 1ms/step - loss: 0.0429 - acc: 0.9864 - val_loss: 0.2322 - val_acc: 0.9271\n",
      "Epoch 4/20\n",
      "50011/50011 [==============================] - 69s 1ms/step - loss: 0.0405 - acc: 0.9860 - val_loss: 0.2472 - val_acc: 0.9382\n",
      "Epoch 5/20\n",
      "50011/50011 [==============================] - 69s 1ms/step - loss: 0.0390 - acc: 0.9863 - val_loss: 0.2948 - val_acc: 0.9256\n",
      "Epoch 6/20\n",
      "50011/50011 [==============================] - 69s 1ms/step - loss: 0.0329 - acc: 0.9898 - val_loss: 0.2935 - val_acc: 0.9210\n",
      "Epoch 7/20\n",
      "50011/50011 [==============================] - 69s 1ms/step - loss: 0.0306 - acc: 0.9911 - val_loss: 0.3341 - val_acc: 0.9179\n",
      "Epoch 8/20\n",
      "50011/50011 [==============================] - 69s 1ms/step - loss: 0.0304 - acc: 0.9911 - val_loss: 0.3470 - val_acc: 0.9144\n",
      "Epoch 9/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0314 - acc: 0.9892 - val_loss: 0.2643 - val_acc: 0.9176\n",
      "Epoch 10/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0501 - acc: 0.9848 - val_loss: 0.2146 - val_acc: 0.9431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0750 - acc: 0.9834 - val_loss: 0.2544 - val_acc: 0.9357\n",
      "Epoch 12/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0793 - acc: 0.9837 - val_loss: 0.2549 - val_acc: 0.9356\n",
      "Epoch 13/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0907 - acc: 0.9780 - val_loss: 0.2443 - val_acc: 0.9448\n",
      "Epoch 14/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0700 - acc: 0.9831 - val_loss: 0.2483 - val_acc: 0.9361\n",
      "Epoch 15/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0477 - acc: 0.9840 - val_loss: 0.2597 - val_acc: 0.9433\n",
      "Epoch 16/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0496 - acc: 0.9847 - val_loss: 0.1968 - val_acc: 0.9435\n",
      "Epoch 17/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0458 - acc: 0.9856 - val_loss: 0.2388 - val_acc: 0.9404\n",
      "Epoch 18/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0429 - acc: 0.9862 - val_loss: 0.2298 - val_acc: 0.9478\n",
      "Epoch 19/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0424 - acc: 0.9869 - val_loss: 0.2133 - val_acc: 0.9447\n",
      "Epoch 20/20\n",
      "50011/50011 [==============================] - 68s 1ms/step - loss: 0.0396 - acc: 0.9860 - val_loss: 0.2228 - val_acc: 0.9464\n",
      "Going for USER  4\n",
      "Training samples:  118493 \n",
      "Test samples:       45675 \n",
      "Features:             110\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (39476, 64, 110) \n",
      "Dataset of Labels have shape:    (39476, 2) \n",
      "Fraction of labels:   [0.08698956 0.91301044]\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (15204, 64, 110) \n",
      "Dataset of Labels have shape:    (15204, 2) \n",
      "Fraction of labels:   [0.14785583 0.85214417]\n",
      "Train on 39476 samples, validate on 15204 samples\n",
      "Epoch 1/20\n",
      "39476/39476 [==============================] - 56s 1ms/step - loss: 0.0726 - acc: 0.9756 - val_loss: 0.1517 - val_acc: 0.9571\n",
      "Epoch 2/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0499 - acc: 0.9845 - val_loss: 0.1772 - val_acc: 0.9515\n",
      "Epoch 3/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0608 - acc: 0.9798 - val_loss: 0.1773 - val_acc: 0.9547\n",
      "Epoch 4/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0590 - acc: 0.9811 - val_loss: 0.1659 - val_acc: 0.9527\n",
      "Epoch 5/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0537 - acc: 0.9832 - val_loss: 0.1551 - val_acc: 0.9556\n",
      "Epoch 6/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0465 - acc: 0.9868 - val_loss: 0.1574 - val_acc: 0.9611\n",
      "Epoch 7/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0440 - acc: 0.9868 - val_loss: 0.1462 - val_acc: 0.9646\n",
      "Epoch 8/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0386 - acc: 0.9888 - val_loss: 0.1448 - val_acc: 0.9643\n",
      "Epoch 9/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0406 - acc: 0.9884 - val_loss: 0.1561 - val_acc: 0.9614\n",
      "Epoch 10/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0439 - acc: 0.9875 - val_loss: 0.1847 - val_acc: 0.9502\n",
      "Epoch 11/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0430 - acc: 0.9866 - val_loss: 0.1897 - val_acc: 0.9559\n",
      "Epoch 12/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0391 - acc: 0.9876 - val_loss: 0.1782 - val_acc: 0.9561\n",
      "Epoch 13/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0333 - acc: 0.9901 - val_loss: 0.1612 - val_acc: 0.9670\n",
      "Epoch 14/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0375 - acc: 0.9881 - val_loss: 0.1425 - val_acc: 0.9713\n",
      "Epoch 15/20\n",
      "39476/39476 [==============================] - 56s 1ms/step - loss: 0.0341 - acc: 0.9897 - val_loss: 0.1717 - val_acc: 0.9680\n",
      "Epoch 16/20\n",
      "39476/39476 [==============================] - 56s 1ms/step - loss: 0.0345 - acc: 0.9895 - val_loss: 0.1594 - val_acc: 0.9666\n",
      "Epoch 17/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0331 - acc: 0.9888 - val_loss: 0.1750 - val_acc: 0.9659\n",
      "Epoch 18/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0296 - acc: 0.9907 - val_loss: 0.1746 - val_acc: 0.9684\n",
      "Epoch 19/20\n",
      "39476/39476 [==============================] - 55s 1ms/step - loss: 0.0281 - acc: 0.9909 - val_loss: 0.1649 - val_acc: 0.9677\n",
      "Epoch 20/20\n",
      "39476/39476 [==============================] - 56s 1ms/step - loss: 0.0220 - acc: 0.9927 - val_loss: 0.2010 - val_acc: 0.9652\n"
     ]
    }
   ],
   "source": [
    "for s in subjects:\n",
    "    \n",
    "    print(\"Going for USER \", s)\n",
    "    \n",
    "    [x_train, y_train, x_test, y_test, n_classes] = utils.preprocessing(s,\n",
    "                                                                        folder,\n",
    "                                                                        label,\n",
    "                                                                        window_size,\n",
    "                                                                        stride,\n",
    "                                                                        make_binary = True)\n",
    "\n",
    "    input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "    input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1)\n",
    "\n",
    "    detection_model.fit(x = input_train, \n",
    "                   y = y_train, \n",
    "                   epochs = 20, \n",
    "                   batch_size = 300,\n",
    "                   verbose = 1,\n",
    "                   validation_data=(input_test, y_test))\n",
    "\n",
    "detection_model.save('./detection_model_A.h5')\n",
    "detection_model.save_weights('./detection_model_weights_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection_model = load_model('./data/detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_test = detection_model.predict(input_test)\n",
    "prediction = np.argmax(output_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Accuracy: \", accuracy_score(np.argmax(y_test, axis=1), prediction))\n",
    "print(\"F1-measure: \", utils.f1_score(np.argmax(y_test, axis=1), prediction, average='weighted'))\n",
    "\n",
    "cnf_matrix = utils.confusion_matrix(np.argmax(y_test, axis=1), prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(cnf_matrix, classes=[0,1],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
