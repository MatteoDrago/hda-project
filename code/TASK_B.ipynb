{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDA - Project 3\n",
    "## TASK B1: Activity detection\n",
    "This task consists of a binary classification, where a gesture denotes activity and thus the model detects wheter there is a gesture label or not (labeled in column 6).\n",
    "\n",
    "This first cell contains the parameters that can be tuned for code execution:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- label_col: index of feature column to be selected to perform activity detection, between [0,6];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 1\n",
    "folder = \"./data/full/\"\n",
    "label = 0     # default for task B1\n",
    "window_size = 15\n",
    "stride = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import deeplearning\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  157125 \n",
      "Test samples:       57536 \n",
      "Features:             110\n",
      "Classes: 2\n",
      "\n",
      "TRAINING SET:\n",
      "Dataset of Images have shape:  (31422, 15, 110) \n",
      "Dataset of Labels have shape:    (31422, 2) \n",
      "Fraction of labels:   [0.11036853 0.88963147]\n",
      "\n",
      "TEST SET:\n",
      "Dataset of Images have shape:  (11504, 15, 110) \n",
      "Dataset of Labels have shape:    (11504, 2) \n",
      "Fraction of labels:   [0.1772427 0.8227573]\n"
     ]
    }
   ],
   "source": [
    "[x_train, y_train, x_test, y_test, n_features, n_classes] = utils.preprocessing(subject,\n",
    "                                                                                folder,\n",
    "                                                                                label,\n",
    "                                                                                window_size,\n",
    "                                                                                stride,\n",
    "                                                                                printInfo = True,\n",
    "                                                                                make_binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of data in a input-suitable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv1d_1: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d259b6727ac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdetection_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeeplearning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdetection_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# model visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\hda-project\\code\\deeplearning.py\u001b[0m in \u001b[0;36mModel1D\u001b[1;34m(input_shape, classes)\u001b[0m\n\u001b[0;32m    222\u001b[0m                     \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                     \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                     input_shape = input_shape))\n\u001b[0m\u001b[0;32m    225\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    465\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    470\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    473\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer conv1d_1: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "detection_model = deeplearning.Model1D((window_size,n_features,1), n_classes)\n",
    "detection_model.summary() # model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    return 1 - utils.f1_score(np.argmax(y_true), y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detection_model.compile(optimizer = Adam(lr=0.01), \n",
    "                        loss = \"categorical_crossentropy\",\n",
    "                        metrics = [\"accuracy\"])\n",
    "                        \n",
    "\n",
    "input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/weights_d.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "detection_model.fit(x = input_train, \n",
    "                    y = y_train, \n",
    "                    epochs = 5, \n",
    "                    batch_size = 128,\n",
    "                    verbose = 1,\n",
    "                    validation_data=(input_test, y_test),\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = detection_model.predict(input_test)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "print(classification_report(y_test, to_categorical(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model_best = load_model('./data/weights_d.hdf5')\n",
    "\n",
    "y_pred = detection_model_best.predict(input_test)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "print(classification_report(y_test, to_categorical(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B2: gesture recognition\n",
    "This task consists of a 17-class classification, where gestures are labeled in column 6.\n",
    "\n",
    "To tune the following parameters, refer to the first cell of task B1:\n",
    "- subject: select the subject on which to test the model, between [1,4];\n",
    "- folder: directory name where '.mat' files are stored;\n",
    "- label_col: index of feature column to be selected to perform activity detection, between [0,6];\n",
    "- window_size: parameter that sets the length of temporal windows on which to perform the convolution;\n",
    "- stride: step length to chose the next window.\n",
    "\n",
    "Here we just need to preserve the different labels, thus we set 'make_binary' to False. We have then 18 different labels, keeping into account the null class, with label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_train, y_train, x_test, y_test, n_features, n_classes] = utils.preprocessing(subject,\n",
    "                                                                    folder,\n",
    "                                                                    label,\n",
    "                                                                    window_size,\n",
    "                                                                    stride,\n",
    "                                                                    printInfo = True,\n",
    "                                                                    make_binary = False,\n",
    "                                                                    null_class = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 4 # OVERWRITE TO BE FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = deeplearning.Model1D((window_size,n_features,1), n_classes)\n",
    "classification_model.summary() # model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.compile(optimizer = Adam(lr=0.01), \n",
    "                             loss = \"categorical_crossentropy\", \n",
    "                             metrics = [\"accuracy\"])\n",
    "\n",
    "input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/weights_c.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "classification_model.fit(x = input_train, \n",
    "                         y = y_train, \n",
    "                         epochs = 10, \n",
    "                         batch_size = 128,\n",
    "                         verbose = 1,\n",
    "                         validation_data=(input_test, y_test),\n",
    "                         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classification_model.predict(input_test)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "print(classification_report(y_test, to_categorical(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model_best = load_model('./data/weights_c.hdf5')\n",
    "\n",
    "y_pred = detection_model_best.predict(input_test)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "print(classification_report(y_test, to_categorical(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with null class\n",
    "(detection and classification are performed together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_train, y_train, x_test, y_test, n_features, n_classes] = utils.preprocessing(subject,\n",
    "                                                                                folder,\n",
    "                                                                                label,\n",
    "                                                                                window_size,\n",
    "                                                                                stride,\n",
    "                                                                                printInfo = True,\n",
    "                                                                                make_binary = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = deeplearning.Model1D((window_size,n_features,1), n_classes)\n",
    "classification_model.summary() # model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.compile(optimizer = Adam(lr=0.01), \n",
    "                             loss = \"categorical_crossentropy\", \n",
    "                             metrics = [\"accuracy\"])\n",
    "\n",
    "input_train = x_train.reshape(x_train.shape[0], window_size, n_features, 1)\n",
    "input_test = x_test.reshape(x_test.shape[0], window_size, n_features, 1)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./data/weights_dc.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "classification_model.fit(x = input_train, \n",
    "                         y = y_train, \n",
    "                         epochs = 10, \n",
    "                         batch_size = 300,\n",
    "                         verbose = 1,\n",
    "                         validation_data=(input_test, y_test),\n",
    "                         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classification_model.predict(input_test)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "print(classification_report(y_test, to_categorical(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_best = load_model('./data/weights_dc.hdf5')\n",
    "\n",
    "y_pred = classification_model_best.predict(input_test)\n",
    "y_pred = np.argmax(y_pred, 1)\n",
    "\n",
    "print(classification_report(y_test, to_categorical(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
